



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.6">
    
    
      
        <title>Deepwalk等 - MOLU</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#009688">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="teal" data-md-color-accent="pink">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#1deepwalk" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="MOLU" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                MOLU
              </span>
              <span class="md-header-nav__topic">
                Deepwalk等
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="主页" class="md-tabs__link">
        主页
      </a>
    
  </li>

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../a_算法/" title="机器与深度学习" class="md-tabs__link">
          机器与深度学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../new_error/" title="问题与分析" class="md-tabs__link">
          问题与分析
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../c视频经典算法/" title="经典算法" class="md-tabs__link">
          经典算法
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="./" title="高级算法" class="md-tabs__link md-tabs__link--active">
          高级算法
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="MOLU" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    MOLU
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器与深度学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器与深度学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../a_算法/" title="数据分析算法" class="md-nav__link">
      数据分析算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../a_cnn/" title="CNN" class="md-nav__link">
      CNN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../a_百度技术认证/" title="百度技术认证" class="md-nav__link">
      百度技术认证
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      问题与分析
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        问题与分析
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../new_error/" title="方差,偏差，噪声" class="md-nav__link">
      方差,偏差，噪声
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_modern/" title="模型评估方法和性能指标" class="md-nav__link">
      模型评估方法和性能指标
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_sigmoid/" title="激活函数与梯度消失，梯度爆炸" class="md-nav__link">
      激活函数与梯度消失，梯度爆炸
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_test/" title="验证集与测试集" class="md-nav__link">
      验证集与测试集
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_transfer/" title="欠拟合，过拟合，正则化" class="md-nav__link">
      欠拟合，过拟合，正则化
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      经典算法
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        经典算法
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../c视频经典算法/" title="蓝桥杯基础" class="md-nav__link">
      蓝桥杯基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c蓝桥杯练习题/" title="蓝桥杯题" class="md-nav__link">
      蓝桥杯题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c背包问题/" title="背包问题" class="md-nav__link">
      背包问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c_leetcode/" title="leetcode" class="md-nav__link">
      leetcode
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      高级算法
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        高级算法
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Deepwalk等
      </label>
    
    <a href="./" title="Deepwalk等" class="md-nav__link md-nav__link--active">
      Deepwalk等
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1deepwalk" title="1.DEEPWALK:" class="md-nav__link">
    1.DEEPWALK:
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#truncated-random-walk" title="截断随机游走(truncated random walk)" class="md-nav__link">
    截断随机游走(truncated random walk)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2deepwalk" title="2.【论文笔记】DeepWalk" class="md-nav__link">
    2.【论文笔记】DeepWalk
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-definition" title="Problem Definition" class="md-nav__link">
    Problem Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-social-representations" title="Learning social representations" class="md-nav__link">
    Learning social representations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#algorithm" title="Algorithm" class="md-nav__link">
    Algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_1" title="总结" class="md-nav__link">
    总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-word2vec-skip-gram" title="3.一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" class="md-nav__link">
    3.一文详解 Word2vec 之 Skip-Gram 模型（结构篇）
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word2vecembeddings" title="什么是Word2Vec和Embeddings？" class="md-nav__link">
    什么是Word2Vec和Embeddings？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="模型" class="md-nav__link">
    模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-fake-task" title="The Fake Task" class="md-nav__link">
    The Fake Task
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="模型细节" class="md-nav__link">
    模型细节
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" title="隐层---查表" class="md-nav__link">
    隐层---查表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="输出层" class="md-nav__link">
    输出层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="直觉上的理解" class="md-nav__link">
    直觉上的理解
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../d_GAN/" title="GAN" class="md-nav__link">
      GAN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../d_GCN论文解读/" title="GCN" class="md-nav__link">
      GCN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../d_GAT/" title="GAT" class="md-nav__link">
      GAT
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1deepwalk" title="1.DEEPWALK:" class="md-nav__link">
    1.DEEPWALK:
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#truncated-random-walk" title="截断随机游走(truncated random walk)" class="md-nav__link">
    截断随机游走(truncated random walk)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2deepwalk" title="2.【论文笔记】DeepWalk" class="md-nav__link">
    2.【论文笔记】DeepWalk
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" title="Introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-definition" title="Problem Definition" class="md-nav__link">
    Problem Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-social-representations" title="Learning social representations" class="md-nav__link">
    Learning social representations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#algorithm" title="Algorithm" class="md-nav__link">
    Algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_1" title="总结" class="md-nav__link">
    总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-word2vec-skip-gram" title="3.一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" class="md-nav__link">
    3.一文详解 Word2vec 之 Skip-Gram 模型（结构篇）
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word2vecembeddings" title="什么是Word2Vec和Embeddings？" class="md-nav__link">
    什么是Word2Vec和Embeddings？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="模型" class="md-nav__link">
    模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-fake-task" title="The Fake Task" class="md-nav__link">
    The Fake Task
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="模型细节" class="md-nav__link">
    模型细节
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" title="隐层---查表" class="md-nav__link">
    隐层---查表
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="输出层" class="md-nav__link">
    输出层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="直觉上的理解" class="md-nav__link">
    直觉上的理解
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Deepwalk等</h1>
                
                <h2 id="1deepwalk">1.DEEPWALK:<a class="headerlink" href="#1deepwalk" title="Permanent link">&para;</a></h2>
<p>随机游走</p>
<p>随机游走是一种从图中提取序列的技术。我们可以使用这些序列来训练一个skip-gram模型来学习节点嵌入。</p>
<p>让我说明一下随机游走的工作原理。让我们考虑下面的无向图：</p>
<p><img alt="img" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/103433-722970.jpeg" /></p>
<p>我们将在该图上应用随机游走并从中提取节点序列。我们将从节点1开始，并覆盖任意方向的两条边：</p>
<p><img alt="img" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/103441-715849.jpeg" /></p>
<p>从节点1，我们可以转到任何连接的节点（节点3或节点4）。我们随机选择了节点4。现在再次从节点4开始，我们不得不随机选择前进的方向。我们将转到节点5。现在我们有3个节点的序列：[节点1 –节点4 –节点5]。</p>
<p>让我们生成另一个序列，但是这次是从另一个节点生成的：</p>
<p><img alt="img" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/103443-960274.jpeg" /></p>
<p>让我们选择节点15作为原始节点。从节点5和6，我们将随机选择节点6。然后从节点11和2，我们选择节点2。新序列为[节点15 –节点6 –节点2]。</p>
<p>我们将对图中的每个节点重复此过程。这就是随机游走技术的工作原理。</p>
<p>在生成节点序列之后，我们必须将它们提供给一个skip-gram模型以获得节点嵌入。整个过程被称为Deepwalk。</p>
<h3 id="truncated-random-walk"><strong>截断随机游走</strong>(truncated random walk)<a class="headerlink" href="#truncated-random-walk" title="Permanent link">&para;</a></h3>
<p>DeepWalk将随机游走得到的节点序列当做句子，从截断的随机游走序列中得到网络的局部信息，再通过局部信息来学习节点的潜在表示。</p>
<p>截断随机游走(truncated random walk)实际上就是长度固定的随机游走。</p>
<p>学习节点嵌入的两个重要的现代算法是DeepWalk和Node2Vec</p>
<p>上面讨论的特征仅包含与节点有关的信息。它们不捕获有关节点上下文的信息。在上下文中，我指的**是周围的节点**。节点嵌入通过用固定长度向量表示每个节点，在一定程度上解决了这个问题。这些向量能够捕获有关周围节点的信息</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-c4e2a874274779c33c511ce06f11283e_720w.jpg" /></p>
<p>上图为用来向量化的神经网络</p>
<p>还是暂时先将10000个点用长度10000的向量表示（如顶点1表示为[1,0,0,...,0] 顶点4为[0,0,0,1,0,...,0]</p>
<p>输入为**某个顶点**</p>
<p><strong>通过10000*h的W矩阵 变成了h长度的向量（h怎么设定？）</strong></p>
<p>再通过h*10000的矩阵变回10000长度的向量</p>
<p>我们通过deepwalk得到了许多的“句子” 即为我们的训练集</p>
<p>当网络训练成熟后，形象理解，我们已经将每个vertex的特征刻画在了neural network里的参数中</p>
<p>观察上图标蓝处，input的第一个维度只和W1的第一行进行了线性计算</p>
<p>也就是W1的第一行便可以提取input的第一个维度的信息 也就是顶点1的信息</p>
<p>所以顶点1的低维度向量表示即为W1的第一行</p>
<p>最后需注意的是h的值要设置较小</p>
<p>在paper里 作者为了生动表达效果将h设置为2</p>
<p>结果如图</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-5589a6dc30da1736eed50963dc17b8d3_720w.jpg" /></p>
<p>要训练这个网络 我们需要设置目标</p>
<p>我们还可以在每个节点的图中捕获此类上下文信息。但是，为了学习NLP空间中的词嵌入，我们将句子提供给**Skip-gram模型（浅层神经网络**）。句子是按一定顺序排列的单词序列。</p>
<p><a href="https://baijiahao.baidu.com/s?id=1651887093786914761&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1651887093786914761&amp;wfr=spider&amp;for=pc</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/?utmsource=blog&amp;utmmedium=graph-feature-extraction-deepwalk">https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/?utmsource=blog&amp;utmmedium=graph-feature-extraction-deepwalk</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/58105731">https://zhuanlan.zhihu.com/p/58105731</a></p>
<h2 id="2deepwalk">2.【论文笔记】DeepWalk<a class="headerlink" href="#2deepwalk" title="Permanent link">&para;</a></h2>
<p><a href="https://www.zhihu.com/people/wang-sheng-4-42"><img alt="陌上疏影凉" src="https://pic4.zhimg.com/v2-0b6c921f36c16ea37a8b56196dba0d8d_xs.jpg" /></a></p>
<p><a href="https://www.zhihu.com/people/wang-sheng-4-42">陌上疏影凉</a></p>
<p>一条咸鱼</p>
<p>关注他</p>
<p>128 人赞同了该文章</p>
<blockquote>
<p>论文名称：DeepWalk: Online Learning of Social Representations</p>
</blockquote>
<p>本文是第一个将NLP中的思想用在网络嵌入(Network Embedding,NE)上的。</p>
<h3 id="introduction"><strong>Introduction</strong><a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h3>
<p>文章简介部分介绍了网络嵌入是什么，以社交网络为例，网络嵌入就是将网络中的点用一个低维的向量表示，并且这些向量要能反应原先网络的某些特性，比如如果在原网络中两个点的结构类似，那么这两个点表示成的向量也应该类似。</p>
<p>本文提出了一种网络嵌入的方法叫**DeepWalk**，它的输入是一张图或者网络，输出为网络中顶点的向量表示。DeepWalk通过**截断随机游走**(truncated random walk)学习出一个网络的**社会表示**(social representation)，在网络标注顶点很少的情况也能得到比较好的效果。并且该方法还具有可扩展的优点，能够适应网络的变化。</p>
<h3 id="problem-definition"><strong>Problem Definition</strong><a class="headerlink" href="#problem-definition" title="Permanent link">&para;</a></h3>
<p>图G定义如下，由顶点集V和边集E组成：</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-68128d0d78b8d54807b0d0c694952b38_720w.jpg" /></p>
<p>如果在图G的基础上再加上顶点的向量表示和顶点所属的标注（网络节点分类问题中，网络中的每个顶点都有一个类别，所属的类别即为该顶点的标注）就构成了一个**标注图**(labeled graph)。顶点的表示X是一个|V|×s维的矩阵，|V|表示顶点的数量，s是代表每个顶点的向量的维数（一般比较小），所以X即为将每个顶点的向量结合在一起形成的矩阵**。Y则是每个顶点的标注构成的矩阵。**（这个含义并不懂）</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-ba80119413549c7592d4da67af91d413_720w.jpg" /></p>
<h3 id="learning-social-representations"><strong>Learning social representations</strong><a class="headerlink" href="#learning-social-representations" title="Permanent link">&para;</a></h3>
<p>文中提到，在学习一个网络表示的时候需要注意的几个性质：</p>
<ul>
<li><strong>适应性</strong>，网络表示必须能适应网络的变化。网络是一个动态的图，不断地会有新的节点和边添加进来，网络表示需要适应网络的正常演化。</li>
<li><strong>属于同一个社区的节点有着类似的表示</strong>。网络中往往会出现一些特征相似的点构成的团状结构，这些节点表示成向量后必须相似。</li>
<li><strong>低维</strong>。代表每个顶点的向量维数不能过高，过高会有过拟合的风险，对网络中有缺失数据的情况处理能力较差。</li>
<li><strong>连续性</strong>。低维的向量应该是连续的。</li>
</ul>
<p>提到网络嵌入，可能会让人联想到NLP中的word2vec，也就是**词嵌入**(word embedding)。前者是将网络中的节点用向量表示，后者是将单词用向量表示。因为大多数机器学习的方法的输入往往都是一个向量，算法也都基于对向量的处理，从而将不能直接处理的东西转化成向量表示，这样就能利用机器学习的方法对其分析，这是一种很自然的思想。</p>
<p>本文处理网络节点的表示(node representation)就是利用了词嵌入（词向量）的的思想。词嵌入的基本处理元素是**单词**，对应网络网络节点的表示的处理元素是**网络节点**；词嵌入是对构成一个**句子**中单词序列进行分析，那么网络节点的表示中节点构成的序列就是**随机游走**。</p>
<p>所谓随机游走(random walk)，就是在网络上不断重复地随机选择游走路径，最终形成一条贯穿网络的路径。从某个特定的端点开始，游走的每一步都从与当前节点相连的边中随机选择一条，沿着选定的边移动到下一个顶点，不断重复这个过程。下图所示绿色部分即为一条随机游走。</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-b96923a3237c666e7b4ed58c62ed903a_720w.jpg" /></p>
<p>关于随机游走的符号解释：以 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> 为根节点生成的一条随机游走路径（绿色）为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=W_%7Bv_i%7D" /> ，其中路径上的点（蓝色）分别标记为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=W%5E1_%7Bv_i%7D%2CW%5E2_%7Bv_i%7D%2CW%5E3_%7Bv_i%7D" /> …，截断随机游走(truncated random walk)实际上就是长度固定的随机游走。</p>
<p>使用随机游走有两个好处：</p>
<ul>
<li><strong>并行化</strong>，随机游走是局部的，对于一个大的网络来说，可以同时在不同的顶点开始进行一定长度的随机游走，多个随机游走同时进行，可以减少采样的时间。</li>
<li><strong>适应性</strong>，可以适应网络局部的变化。网络的演化通常是局部的点和边的变化，这样的变化只会对部分随机游走路径产生影响，因此在网络的演化过程中不需要每一次都重新计算整个网络的随机游走。</li>
</ul>
<p>文中提到网络中随机游走的分布规律与NLP中句子序列在语料库中出现的规律有着类似的幂律分布特征。那么既然网络的特性与自然语言处理中的特性十分类似，<strong>那么就可以将NLP中词向量的模型用在网络表示中，这正是本文所做的工作。</strong></p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-eb696bb3c340f09c7ce829f641fddfde_720w.jpg" /></p>
<p>首先来看词向量模型：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=w%5Eu_i+%3D+%28w_0%2Cw_1%2Cw_2%2C%E2%80%A6%2Cw_n%29" /> 是一个由若干单词组成的序列，其中 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=w_i%E2%88%88V%28Vocabulary%29%EF%BC%8CV" /> 是词汇表，也就是所有单词组成的集合。</p>
<p>在整个训练集上需要优化的目标是：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=Pr%28w_n%7Cw_0%2Cw_1%2C%E2%80%A6%2Cw_n-1%29" /></p>
<p>比如下面的单词组成的序列：</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-b85fc0530f77de563272659581317f26_720w.jpg" /></p>
<p>优化目标就是 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=Pr%28w_3%7Cw_0%2Cw_1%2Cw_2%29" /> ，意思就是当知道I like studying后，下一个词是English的概率为多少？</p>
<p>如果将单词对应成网络中的节点 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> ，句子序列对应成网络的随机游走，那么对于一个随机游走 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%28v_0%2Cv_1%2C%E2%80%A6%2Cv_i%29" /> 要优化的目标就是：</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=Pr%28v_i%7C%28v_0%2Cv_1%2C%E2%80%A6%2Cv_%7Bi-1%7D%29%29" /></p>
<p>按照上面的理解就是，当知道 <strong><img alt="[公式]" src="https://www.zhihu.com/equation?tex=%28v_0%2Cv_1%2C%E2%80%A6%2Cv_%7Bi-1%7D%29" /> 游走路径后</strong>（是一个路径），游走的下一个节点是 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> 的概率是多少？可是这里的 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> 是顶点本身没法计算，于是引入一个映射函数 ，它的功能是**将顶点映射成向量（这其实就是我们要求的），转化成向量后就可以对顶点 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> 进行计算了。**</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-56d93498730a54286abdb3f05c43be42_720w.jpg" /></p>
<p>映射函数 对网络中每一个节点映射成d维向量， 实际上是一个矩阵，总共有|V|×d个参数，<strong>这些参数就是需要学习的。</strong></p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-87ad14abc69d8a572bf548b7bc79200a_720w.jpg" /></p>
<p>有了 之后， <img alt="[公式]" src="https://www.zhihu.com/equation?tex=+%28v_i%29" /> 就是一个可以计算的向量了，这时原先的优化目标可以写成：这里的（vi)表示对应映射的向量</p>
<p><img alt="[公式]" src="https://www.zhihu.com/equation?tex=Pr%28v_i%7C%28+%28v_0%29%2C+%28v_1%29%2C%E2%80%A6%2C+%28%7Bv_i-1%7D%29%29%29" /></p>
<p>**但是怎么计算这个概率呢？**同样借用词向量中使用的skip-gram模型</p>
<p>skip-gram模型有这样3个特点：</p>
<ul>
<li>不使用上下文（context）预测缺失词（missing word），而使用缺失词预测上下文。因为 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=%28+%28v_0%29%2C+%28v_1%29%2C%E2%80%A6%2C+%28%7Bv_i-1%7D%29%29" /> 这部分太难算了，但是如果只计算一个 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=+%28v_k%29" /> ，其中 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_k" /> 是缺失词，这就很好算。</li>
<li>同时考虑左边窗口和右边窗口。下图中橘黄色部分，对于 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_4" /> 同时考虑左边的2个窗口内的节点和右边2个窗口内的节点。</li>
<li>不考虑顺序，只要是窗口中出现的词都算进来，而不管它具体出现在窗口的哪个位置。</li>
</ul>
<p>应用skip-gram模型后，优化目标变成了这样：</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-001ac943d4bc016c24771dfc3ce3712c_720w.jpg" /></p>
<p>其中概率部分的意思是，在一个随机游走中，当给定一个顶点 <img alt="[公式]" src="https://www.zhihu.com/equation?tex=v_i" /> 时，出现它的w窗口范围内顶点的概率。</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-dcf7ea8b54abc865524e39b40465c8b2_720w.jpg" />rw指的是random walk随机游走</p>
<p>做了这样的处理后可以发现，忽视顶点的顺序更好地体现了在随机游走中顶点的邻近关系，并且只需要计算一个顶点的向量，减少了计算量。所以DeepWalk是将截断随机游走与神经语言模型(neural language model)结合形成的网络表示，它具有低维、连续和适应性特征。</p>
<h3 id="algorithm"><strong>Algorithm</strong><a class="headerlink" href="#algorithm" title="Permanent link">&para;</a></h3>
<p>整个DeepWalk算法包含两部分，一部分是随机游走的生成，另一部分是参数的更新。</p>
<p>算法的流程如下：</p>
<p><img alt="img" src="https://pic2.zhimg.com/80/v2-199a580d3a267216a864c9de9a5f3455_720w.jpg" /></p>
<p>其中第2步是构建Hierarchical Softmax，第3步对每个节点做γ次随机游走，第4步打乱网络中的节点，第5步以每个节点为根节点生成长度为t的随机游走，第7步根据生成的随机游走使用skip-gram模型利用梯度的方法对参数进行更新。</p>
<p>参数更新的细节如下：</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-529cf87a8ecb8b63aca9d39dfff0899c_720w.jpg" /></p>
<p>文中还使用了Hierarchical Softmax的方法，这也是词向量中用到的一个重要方法，这里不做赘述，后面考虑再开一篇博客讲讲词向量中用到的技术。</p>
<h3 id="_1"><strong>总结</strong><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>总的来说这篇论文算是network embedding的开山之作，它将NLP中词向量的思想借鉴过来做网络的节点表示，提供了一种新的思路，后面会有好几篇论文使用的也是这种思路，都是利用随机游走的特征构建概率模型，用词向量中Negative Sampling的思想解决相应问题。</p>
<p><a href="https://blog.csdn.net/qq_24003917/article/details/80389976">https://blog.csdn.net/qq_24003917/article/details/80389976</a> </p>
<h2 id="3-word2vec-skip-gram">3.一文详解 Word2vec 之 Skip-Gram 模型（结构篇）<a class="headerlink" href="#3-word2vec-skip-gram" title="Permanent link">&para;</a></h2>
<p>转载<a href="https://me.csdn.net/qq_24003917">qq_24003917</a> 最后发布于2018-05-25 09:40:20 阅读数 8889 收藏</p>
<p>展开</p>
<p>这次的分享主要是对Word2Vec模型的两篇英文文档的翻译、理解和整合，这两篇英文文档都是介绍Word2Vec中的Skip-Gram模型。下一篇专栏文章将会用TensorFlow实现基础版Word2Vec的skip-gram模型，所以本篇文章先做一个理论铺垫。</p>
<p>原文英文文档请参考链接：</p>
<blockquote>
<p>- Word2Vec Tutorial - The Skip-Gram Model</p>
<p><a href="http://t.cn/Rc5RfJ2">http://t.cn/Rc5RfJ2</a></p>
<p>- Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram)</p>
<p><a href="http://t.cn/RoVEiUB">http://t.cn/RoVEiUB</a></p>
</blockquote>
<h3 id="word2vecembeddings">什么是Word2Vec和Embeddings？<a class="headerlink" href="#word2vecembeddings" title="Permanent link">&para;</a></h3>
<p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？<strong>Word2Vec通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近</strong>。<strong>Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</strong></p>
<p>我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：<strong>kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy（幼犬）这个词向量十分相近。</strong></p>
<h3 id="_2">模型<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p>Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。本篇文章仅讲解Skip-Gram模型。</p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://static.leiphone.com/uploads/new/article/740_740/201706/594b306c8b3b1.png?imageMogr2/format/jpg/quality/90" /></p>
<p>Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。</p>
<p>Word2Vec模型实际上分为了两个部分，<strong>第一部分为建立模型，第二部分是通过模型获取嵌入词向量</strong>。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。</p>
<blockquote>
<p>上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。</p>
</blockquote>
<h3 id="the-fake-task">The Fake Task<a class="headerlink" href="#the-fake-task" title="Permanent link">&para;</a></h3>
<p>我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。</p>
<p>接下来我们来看看如何训练我们的神经网络。假如我们有一个句子**“The dog barked at the mailman”**。</p>
<ul>
<li>首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；</li>
<li>有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是**['The', 'dog'，'barked', 'at']<strong>。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 **(input word, output word)</strong> 形式的训练数据，即 <strong>('dog', 'barked')</strong>，<strong>('dog', 'the')</strong>。</li>
<li>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 <strong>('dog', 'barked')</strong> 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。</li>
</ul>
<p>模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子**“The quick brown fox jumps over lazy dog”**，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。</p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://static.leiphone.com/uploads/new/article/740_740/201706/594b319eb5f1f.png?imageMogr2/format/jpg/quality/90" /></p>
<p>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。</p>
<h3 id="_3">模型细节<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行**one-hot编码**。</p>
<p>假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，<strong>得到的每个单词都是一个10000维的向量</strong>，<strong>向量每个维度的值只有0或者1</strong>，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（ants=[0, 0, 1, 0, ..., 0]）。</p>
<p>模型**的输入如果为一个10000维的向量**，<strong>那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。</strong></p>
<p>下图是我们神经网络的结构：</p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/105239-464862.jpeg" /></p>
<p>隐层没有使用任何激活函数，但是输出层使用了sotfmax。</p>
<p>我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。</p>
<h3 id="-">隐层---查表<a class="headerlink" href="#-" title="Permanent link">&para;</a></h3>
<p>说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用**300个特征来表示一个单词**（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。</p>
<p>Google在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。**词向量的维度是一个可以调节的超参数（**在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。</p>
<p>看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。<strong>从右边的图来看，每一行实际上代表了每个单词的词向量</strong>。</p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/105230-281252.jpeg" /></p>
<p><strong>所以我们最终的目标就是学习这个隐层的权重矩阵。</strong></p>
<p>我们现在回来接着通过模型的定义来训练我们的这个模型。</p>
<p>上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。</p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/105248-809027.jpeg" /></p>
<p>我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为0 x 17 + 0 x 23 + 0 x 4 + 1 x 10 + 0 x 11 = 10，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。</p>
<p>为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，<strong>这样模型中的隐层权重矩阵便成了一个”查找表“</strong>（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。<strong>隐层的输出就是每个输入单词的“嵌入词向量”。</strong></p>
<h3 id="_4">输出层<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。</p>
<p>下面是一个例子**，训练样本为 (input word: “ants”， output word: “car”)** 的计算示意图。</p>
<p><strong>总结：</strong></p>
<p><img alt="一文详解 Word2vec 之 Skip-Gram 模型（结构篇）" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/105252-875692.jpeg" /></p>
<p><img alt="在这里插入图片描述" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/111604-277966.png" /></p>
<p><img alt="image-20200716112027562" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/112028-826649.png" /></p>
<p><img alt="image-20200716112044089" src="https://gitee.com/moluggg/image/raw/master/img/202007/16/112045-875747.png" /></p>
<p>摘录于：<a href="https://blog.csdn.net/weixin_41843918/article/details/90312339">https://blog.csdn.net/weixin_41843918/article/details/90312339</a></p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> Deepwalk更新的是哪一个参数？w和w'都更新吗？</p>
<h3 id="_5">直觉上的理解<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>下面我们将通过直觉来进行一些思考。</p>
<p>如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。</p>
<p>那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。</p>
<p>实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。</p>
<blockquote>
<p>词干化（stemming）就是去除词缀得到词根的过程。</p>
</blockquote>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../c_leetcode/" title="leetcode" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                leetcode
              </span>
            </div>
          </a>
        
        
          <a href="../d_GAN/" title="GAN" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                GAN
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.5e60981f.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="../js/extra.js"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>