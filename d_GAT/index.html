



<!DOCTYPE html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-3.0.6">
    
    
      
        <title>GAT - MOLU</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
      
      
        
        
        <meta name="theme-color" content="#009688">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="teal" data-md-color-accent="pink">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#directional-adversarial-training-for-recommender-systems" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="MOLU" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                MOLU
              </span>
              <span class="md-header-nav__topic">
                GAT
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="主页" class="md-tabs__link">
        主页
      </a>
    
  </li>

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../a_算法/" title="机器与深度学习" class="md-tabs__link">
          机器与深度学习
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../new_error/" title="问题与分析" class="md-tabs__link">
          问题与分析
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../c视频经典算法/" title="经典算法" class="md-tabs__link">
          经典算法
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../d_Deepwalk/" title="高级算法" class="md-tabs__link md-tabs__link--active">
          高级算法
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="MOLU" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    MOLU
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="主页" class="md-nav__link">
      主页
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      机器与深度学习
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        机器与深度学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../a_算法/" title="数据分析算法" class="md-nav__link">
      数据分析算法
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../a_cnn/" title="CNN" class="md-nav__link">
      CNN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../a_百度技术认证/" title="百度技术认证" class="md-nav__link">
      百度技术认证
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      问题与分析
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        问题与分析
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../new_error/" title="方差,偏差，噪声" class="md-nav__link">
      方差,偏差，噪声
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_modern/" title="模型评估方法和性能指标" class="md-nav__link">
      模型评估方法和性能指标
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_sigmoid/" title="激活函数与梯度消失，梯度爆炸" class="md-nav__link">
      激活函数与梯度消失，梯度爆炸
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_test/" title="验证集与测试集" class="md-nav__link">
      验证集与测试集
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../new_transfer/" title="欠拟合，过拟合，正则化" class="md-nav__link">
      欠拟合，过拟合，正则化
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      经典算法
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        经典算法
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../c视频经典算法/" title="蓝桥杯基础" class="md-nav__link">
      蓝桥杯基础
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c蓝桥杯练习题/" title="蓝桥杯题" class="md-nav__link">
      蓝桥杯题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c背包问题/" title="背包问题" class="md-nav__link">
      背包问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../c_leetcode/" title="leetcode" class="md-nav__link">
      leetcode
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      高级算法
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        高级算法
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../d_Deepwalk/" title="Deepwalk等" class="md-nav__link">
      Deepwalk等
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../d_GAN/" title="GAN" class="md-nav__link">
      GAN
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../d_GCN论文解读/" title="GCN" class="md-nav__link">
      GCN
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        GAT
      </label>
    
    <a href="./" title="GAT" class="md-nav__link md-nav__link--active">
      GAT
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#directional-adversarial-training-for-recommender-systems" title="Directional Adversarial Training for Recommender Systems" class="md-nav__link">
    Directional Adversarial Training for Recommender Systems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" title="1.摘要:" class="md-nav__link">
    1.摘要:
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" title="对抗性训练的引入以及现阶段存在的缺点：" class="md-nav__link">
    对抗性训练的引入以及现阶段存在的缺点：
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" title="2. 相关工作" class="md-nav__link">
    2. 相关工作
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-adversarial-training" title="2.2.1 Adversarial Training 对抗性训练" class="md-nav__link">
    2.2.1 Adversarial Training 对抗性训练
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-gan" title="2.2.2 GAN" class="md-nav__link">
    2.2.2 GAN
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-preliminaries" title="3  PRELIMINARIES" class="md-nav__link">
    3  PRELIMINARIES
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-graph-representation" title="**3.1 Graph Representation**图的表示" class="md-nav__link">
    **3.1 Graph Representation**图的表示
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-node-classifification" title="3.2  Node Classifification" class="md-nav__link">
    3.2  Node Classifification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" title="3.3 基于图的学习" class="md-nav__link">
    3.3 基于图的学习
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" title="4.方法：" class="md-nav__link">
    4.方法：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-gat-graph-adversarial-training" title="4.1 GAT （Graph Adversarial Training）" class="md-nav__link">
    4.1 GAT （Graph Adversarial Training）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-virtual-graph-adversarial-training" title="4.2 Virtual Graph Adversarial Training**" class="md-nav__link">
    4.2 Virtual Graph Adversarial Training**
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-gcn" title="4.3  GCN" class="md-nav__link">
    4.3  GCN
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#300-paper" title="300-paper" class="md-nav__link">
    300-paper
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" title="1 引入：" class="md-nav__link">
    1 引入：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apr" title="APR与最大方向的介绍" class="md-nav__link">
    APR与最大方向的介绍
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dat" title="DAT的引入以及扰动策略" class="md-nav__link">
    DAT的引入以及扰动策略
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="本文的主要贡献总结：" class="md-nav__link">
    本文的主要贡献总结：
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" title="2 相关工作" class="md-nav__link">
    2 相关工作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" title="3 初步" class="md-nav__link">
    3 初步
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" title="3.1 问题定义" class="md-nav__link">
    3.1 问题定义
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-bpr" title="3.2 引入BPR：" class="md-nav__link">
    3.2 引入BPR：
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" title="4 措施：" class="md-nav__link">
    4 措施：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-bpr" title="4.1 构造对抗正则项并应用到BPR上：" class="md-nav__link">
    4.1 构造对抗正则项并应用到BPR上：
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-dat" title="4.2 DAT" class="md-nav__link">
    4.2 DAT
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softmax" title="Softmax" class="md-nav__link">
    Softmax
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fgsm" title="FGSM" class="md-nav__link">
    FGSM
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kl-" title="kl 散度---相对熵" class="md-nav__link">
    kl 散度---相对熵
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ngcf" title="NGCF" class="md-nav__link">
    NGCF
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" title="1.背景" class="md-nav__link">
    1.背景
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-methodology" title="2 METHODOLOGY" class="md-nav__link">
    2 METHODOLOGY
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-embedding-layer" title="2.1 Embedding Layer" class="md-nav__link">
    2.1 Embedding Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-embedding-propagation-layers" title="2.2 Embedding Propagation Layers" class="md-nav__link">
    2.2 Embedding Propagation Layers
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-first-order-propagation" title="2.2.1  First-order Propagation" class="md-nav__link">
    2.2.1  First-order Propagation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-high-order-propagation" title="2.2.2  High-order Propagation" class="md-nav__link">
    2.2.2  High-order Propagation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-model-prediction" title="2.3  Model Prediction" class="md-nav__link">
    2.3  Model Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-optimization" title="2.4 Optimization" class="md-nav__link">
    2.4 Optimization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#directional-adversarial-training-for-recommender-systems" title="Directional Adversarial Training for Recommender Systems" class="md-nav__link">
    Directional Adversarial Training for Recommender Systems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" title="1.摘要:" class="md-nav__link">
    1.摘要:
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" title="对抗性训练的引入以及现阶段存在的缺点：" class="md-nav__link">
    对抗性训练的引入以及现阶段存在的缺点：
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" title="2. 相关工作" class="md-nav__link">
    2. 相关工作
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-adversarial-training" title="2.2.1 Adversarial Training 对抗性训练" class="md-nav__link">
    2.2.1 Adversarial Training 对抗性训练
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-gan" title="2.2.2 GAN" class="md-nav__link">
    2.2.2 GAN
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-preliminaries" title="3  PRELIMINARIES" class="md-nav__link">
    3  PRELIMINARIES
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-graph-representation" title="**3.1 Graph Representation**图的表示" class="md-nav__link">
    **3.1 Graph Representation**图的表示
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-node-classifification" title="3.2  Node Classifification" class="md-nav__link">
    3.2  Node Classifification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" title="3.3 基于图的学习" class="md-nav__link">
    3.3 基于图的学习
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" title="4.方法：" class="md-nav__link">
    4.方法：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-gat-graph-adversarial-training" title="4.1 GAT （Graph Adversarial Training）" class="md-nav__link">
    4.1 GAT （Graph Adversarial Training）
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-virtual-graph-adversarial-training" title="4.2 Virtual Graph Adversarial Training**" class="md-nav__link">
    4.2 Virtual Graph Adversarial Training**
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-gcn" title="4.3  GCN" class="md-nav__link">
    4.3  GCN
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#300-paper" title="300-paper" class="md-nav__link">
    300-paper
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" title="1 引入：" class="md-nav__link">
    1 引入：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apr" title="APR与最大方向的介绍" class="md-nav__link">
    APR与最大方向的介绍
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dat" title="DAT的引入以及扰动策略" class="md-nav__link">
    DAT的引入以及扰动策略
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="本文的主要贡献总结：" class="md-nav__link">
    本文的主要贡献总结：
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" title="2 相关工作" class="md-nav__link">
    2 相关工作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" title="3 初步" class="md-nav__link">
    3 初步
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" title="3.1 问题定义" class="md-nav__link">
    3.1 问题定义
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-bpr" title="3.2 引入BPR：" class="md-nav__link">
    3.2 引入BPR：
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" title="4 措施：" class="md-nav__link">
    4 措施：
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-bpr" title="4.1 构造对抗正则项并应用到BPR上：" class="md-nav__link">
    4.1 构造对抗正则项并应用到BPR上：
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-dat" title="4.2 DAT" class="md-nav__link">
    4.2 DAT
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softmax" title="Softmax" class="md-nav__link">
    Softmax
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fgsm" title="FGSM" class="md-nav__link">
    FGSM
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kl-" title="kl 散度---相对熵" class="md-nav__link">
    kl 散度---相对熵
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ngcf" title="NGCF" class="md-nav__link">
    NGCF
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" title="1.背景" class="md-nav__link">
    1.背景
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-methodology" title="2 METHODOLOGY" class="md-nav__link">
    2 METHODOLOGY
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-embedding-layer" title="2.1 Embedding Layer" class="md-nav__link">
    2.1 Embedding Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-embedding-propagation-layers" title="2.2 Embedding Propagation Layers" class="md-nav__link">
    2.2 Embedding Propagation Layers
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-first-order-propagation" title="2.2.1  First-order Propagation" class="md-nav__link">
    2.2.1  First-order Propagation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-high-order-propagation" title="2.2.2  High-order Propagation" class="md-nav__link">
    2.2.2  High-order Propagation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-model-prediction" title="2.3  Model Prediction" class="md-nav__link">
    2.3  Model Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-optimization" title="2.4 Optimization" class="md-nav__link">
    2.4 Optimization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>GAT</h1>
                
                <h3 id="directional-adversarial-training-for-recommender-systems"><strong>Directional Adversarial Training for Recommender</strong> <strong>Systems</strong><a class="headerlink" href="#directional-adversarial-training-for-recommender-systems" title="Permanent link">&para;</a></h3>
<p>论文翻译:</p>
<h3 id="1">1.摘要:<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<p>(话题方向切人,他人最近的有关成果，再次基础上他人成果的不足。提出我们的做法，结果以及引出我们的</p>
<h3 id="_1">对抗性训练的引入以及现阶段存在的缺点：<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<div class="codehilite"><pre><span></span><code>对抗性训练是通过在模型训练过程中在输入空间中进行随机扰动(perturbations)来提高深度学习模型泛化能力的有效方法。 最近的一次 奥迪已经成功地将对抗性训练应用于推荐系统，通过极小极大的游戏干扰用户和项目的嵌入。 然而，这种方法忽略了协同信号 在推荐系统中，未能捕获数据分布中的平滑性。 我们认为，揭示用户和项目之间行为相似性的协作信号是至关重要的建模推荐系统。 在本工作中，我们通过显式地将协作信号注入扰动过程来开发定向对抗性训练(DAT)策略。 即b 用户和项目对嵌入空间中的相似邻居感到不安(perturbed)，并受到适当的限制。 为了验证其有效性，我们演示了DAT在广义矩阵事实上的使用 定向(GMF)是最具代表性的协同过滤方法之一。 我们在三个公共数据集上的实验结果表明，我们的方法(称为DAGMF)达到了显著的精度I对GMF的改进，同时，它比GMF更不容易过度拟合。
</code></pre></div>


<p>对抗性训练是[12]提出的一种新的方法，以解决深度学习模型很容易被对抗性例子[26]愚弄的问题，这些例子是通过施加小的代价来构造的 输入示例上的涡轮。 最近，一些研究[12,22,23]指出，<strong>对抗性训练可以作为一种正则化方法来提高泛化性能 以及防止深度学习模型过度拟合</strong>。</p>
<blockquote>
<p><strong><img alt="🅰" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f170.png" title=":a:" /> 主要内容：</strong></p>
<p>对抗性训练忽略了协同信号未能捕获数据分布中的平滑性，但是作者认为用户和项目之间的行为相似性的写作信号很重要，于是，将写作信号注入到扰动过程并开发定向的对抗训练，斌能够取得了不错的效果。同时学者发现，对抗性训练可以作为正则化提高泛化性能。</p>
</blockquote>
<p>​    尽管性能很好，但我们认为图形神经网络容易受到输入特征[13]、[14]的小而有意的扰动，这甚至可能比不建模图形结构的标准神经网络更为严重。 原因有两个：1)图神经网络也优化了标记数据上的监督损失，因此它将面临相同的损失 作为标准神经网络[15]的生物问题 2)附加的平滑约**束将加剧扰动的影响**，因为跨越连接节点的平滑将聚合来自连接到ta的节点的扰动的影响rget节点（即我们应用扰动以改变其预测的节点）。 图1用图w的直观例子说明了扰动对节点特征的影响有4个节点。 图神经网络模型分别预测具有应用扰动的清洁输入特征和特征的节点标签(总共3个。 在这里，扰动是故意施加的节点1、2、4的特征。 因此，图神经网络模型被愚弄，对节点1和2作出错误的预测，就像标准神经网络一样。 此外，通过传播节点 嵌入，该模型将扰动的影响聚集到节点3，其预测也受到影响。 在实际应用中，小扰动就像节点特性的更新 可能经常发生，但不应太多地改变预测。 因此，我们认为在训练过程中**迫切需要稳定图形神经网络模型。**</p>
<p>对抗性训练(AT)是一种动态正则化技术，它主动模拟训练阶段[15]的扰动。 经验表明它能够稳定神经 在标准分类任务[16]-[21]中增强它们对扰动的鲁棒性。 因此，在图神经网络模型上采用与AT相似的方法也有助于模型的鲁棒性。 然而，直接在图神经网络上使用AT是不够的，因为它将示例视为相互独立的，不考虑来自相关示例的影响。 因此，我们提出了一种新的对抗性训练方法，称为**图形对抗性训练(**Gra 它学习通过考虑图形结构来构造和抵抗扰动。</p>
<blockquote>
<p><img alt="🅰" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f170.png" title=":a:" />这两段的主要内容：</p>
<p>图神经网络也能收到小的扰动的影响，因此也需要在训练过程中迫切需要稳定图形神经网络模型。</p>
</blockquote>
<p>方法是借用对抗性训练的方法，但是又是远远不够的。</p>
<p><img alt="image-20200720110723267" src="E:\typora图片\113807-95698.png" /></p>
<p>图 1：一个直观的例子来说明将扰动应用于输入节点特征对图神经网络预测的影响。 在这里，模型实现了图平滑度c通过在图上传播节点嵌入进行约束。 在右边，模型在目标节点3的连接节点上传播应用的扰动，导致错误的预测。 而且，在节点1和2上的扰动直接导致错误的相关预测，如在标准神经网络中。</p>
<p>GraphAT的关键思想是，当在目标示例 target example上产生扰动时，<strong>它使目标示例的预测与其连接示例（connected examples）之间的分歧最大化</strong>。 也就是说，a对抗性扰动应该**尽可能地攻击图形平滑约束**。 然后，GraphAT通过额外最小化图对抗性正则化(<em>graph adversarial regularizer</em>)来更新模型参数，减少 扰动目标实例与其连通实例之间的预测发散。 通过这种方法，GraphAT可以抵抗基于图形学习的最坏情况扰动，并增强模型半身像。 为了有效地计算对抗性扰动，我们进一步设计了一种基于**反向传播的线性逼近方法**。 </p>
<p>为了演示GraphAT， 我们将其应用于一个建立良好的图形神经网络模型-图形卷积网络(GCN)[7]，该模型通过执行**嵌入传播**(performing embedding propagation)来实现平滑约束。 我们研究美索 节点分类的性能，这是基于图的学习中最受欢迎的任务之一。 我们研究了该方法在节点分类上的性能，这是基于图的学习中最流行的任务之一。 在三个公共基准（两个引文图和一个知识）上进行广泛的实验图)验证了图AT的优势-与GCN的正常训练相比，GraphAT的精度提高了4.51。 此外，对不太受欢迎的节点（有很小的程度）的改进是MOR 重要的，强调了用所考虑的图结构执行AT的必要性。</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> target and connected example是什么意思：</p>
<p>目标节点，以及与目标节点相连接的结点吗？</p>
<p>本文的主要贡献总结为：</p>
<p>我们提出了图形对抗性训练，这是一种新的图形神经网络优化方法，可以提高模型的鲁棒性节点输入特性的扰动。 </p>
<p>我们设计了一个图对抗性正则化器，动态地鼓励模型对扰动目标示例及其连接生成类似的预测举例说明，并开发了一种有效的算法来构造扰动。 </p>
<p>我们证明了图ATON的有效性 在三个数据集上进行了GCN实验，表明我们的方法在节点分类方面达到了最先进的性能.</p>
<p>在本文的其余部分中，我们首先在第二节中讨论相关工作，然后在第三节中讨论问题的制定和准备。 在第4节和第5节中，我们详细阐述了方法和经验 结果，分别。 我们在第六节中总结了这篇论文，并展望了未来的方向。</p>
<h3 id="2">2. 相关工作<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p>在这一部分中，我们讨论了现有的基于图的学习和对抗性学习的研究，这些研究与这项工作密切相关。</p>
<p>图是关系数据的一种自然表示，其中节点和边缘表示实体及其关系，广泛应用于社交网络、交易记录、生物分析中交互、相互关联文档的集合、网页和多媒体内容等。在这样的图表上，最受欢迎的任务之一是节点分类，以预测标签图中的节点通过考虑节点特征和图结构。 现有的节点分类工作主要分为两大类： 图拉普拉斯正则化和基于图嵌入的方法(<em>graph Laplacian regularization</em> and*graph embedding-based methods*)。前一类的方法显式地将图结构编码为正则化项，以平滑图上的预测，i.s.，当用不同的标签预测相似的节点（例如紧密连接）时，正则化会受到很大的惩罚</p>
<p>最近，基于图嵌入的方法，学习编码图数据的节点嵌入，已经成为很有前途的解决方案。 大多数基于嵌入的方法分为两大类：: <em>skip-gram based methods</em> and <em>convolution based methods</em>，取决于图形数据是如何建模的。 skip-gram通过使用节点的嵌入来预测节点conte来学习节点嵌入 通过在图上执行随机游走生成的结点的上下文，以便嵌入“连接”节点 相互关联的[2]、[5]、[6]、[12]。 受计算机视觉中卷积思想的启发，卷积在局部窗口中聚集上下文信号，基于卷积的方法迭代 聚合邻居节点的表示，以学习节点嵌入[3]、[4]、[7]、[11]、[25]-[30]。 </p>
<p>在这两个类别中，利用深度Neural的高级表示能力的方法 网络（基于神经图的学习方法）在解决节点分类任务方面表现出显著的有效性。 然而，<strong>基于神经图的学习模型容易受到意图的影响 设计的扰动表明了泛化[13]、[31]和很少注意的不稳定性，以增强这些方法的鲁棒性，这是本工作的重点。</strong></p>
<h4 id="221-adversarial-training">2.2.1 Adversarial Training 对抗性训练<a class="headerlink" href="#221-adversarial-training" title="Permanent link">&para;</a></h4>
<p>为了解决深层神经网络对有意扰动的脆弱性，<strong>研究人员提出了对抗性训练，这是一种替代的极小极大过程[32]。</strong> 对抗训练 方法通过从具有扰动的干净例子中动态生成对抗性例子来增强训练过程，最大限度地攻击训练目标，然后学习这些通过最小化一个额外的正则化项[15]、[17]、[33]-[38]来证明这些例子。 对抗性训练方法主要分为监督和半监督两种 培训目标。 在有监督的学习任务中，如视觉识别[15]、监督损失[15]、[33]、[34]及其代理[36]-以对抗性例子为目标最大化和最小化。 对于半监督学习，其中部分示例被标记，对每个示例周围的输入的预测作为目标。 一般来说对抗性训练方法的哲学是以动态的方式平滑围绕单个输入的预测。 我们的工作受到了这些对抗性训练方法的启发。 此外 对于单个示例的局部平滑性，我们的方法进一步考虑了极小极大过程目标中的示例（即图结构）之间的关系，以便学习鲁棒分类器在图形结构上平滑地预测。 据我们所知，这是第一次尝试将图形结构纳入对抗性训练。 另一个新兴的研究课题rela 泰德对我们的工作是产生对抗性扰动攻击基于神经图的学习模型，其中[31]和[13]是唯一发表的工作。 然而，[31]和[13]中的方法是不合适的用于构建图对抗性训练中的对抗性示例。 这是因为这些方法生成一个新的图作为每个节点的对抗性示例，即它们将生成 当节点数为N时，N图导致无法负担的内存开销。 在本工作中，我们设计了一种有效的方法来生成用于图形对抗性训练的对抗性示例。</p>
<h4 id="222-gan">2.2.2 GAN<a class="headerlink" href="#222-gan" title="Permanent link">&para;</a></h4>
<h3 id="3-preliminaries">3  PRELIMINARIES<a class="headerlink" href="#3-preliminaries" title="Permanent link">&para;</a></h3>
<h4 id="31-graph-representation">**3.1 Graph Representation**图的表示<a class="headerlink" href="#31-graph-representation" title="Permanent link">&para;</a></h4>
<p><span><span class="MathJax_Preview">G = (A, D, X).</span><script type="math/tex">G = (A, D, X).</script></span></p>
<p>A 表示邻接矩阵矩阵。</p>
<p>D 表示对角矩阵</p>
<p>X表示输入矩阵，</p>
<h4 id="32-node-classifification">3.2  <strong>Node Classifification</strong><a class="headerlink" href="#32-node-classifification" title="Permanent link">&para;</a></h4>
<p>学习预测函数（分类器）<span><span class="MathJax_Preview">y_{i}=f(xi，G|Θ)</span><script type="math/tex">y_{i}=f(xi，G|Θ)</script></span>，预测节点的标签，其中Θ包括要学习的模型参数。</p>
<p>其中<span><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span>表示结点的标签</p>
<h4 id="33">3.3 基于图的学习<a class="headerlink" href="#33" title="Permanent link">&para;</a></h4>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 这一部分就是讲的GCN是吧？
$$
\Gamma=\Omega+\lambda \Phi
$$
 Φ鼓励在图结构上预测的平滑性，这是基于这样的假设，即紧密连接的节点往往相似的预测</p>
<p>该公式具体表示为：(引用了原始GCN文章中的代价函数)</p>
<div>
<div class="MathJax_Preview">
\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad \text { with } \quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X)
</div>
<script type="math/tex; mode=display">
\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad \text { with } \quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X)
</script>
</div>
<h3 id="4">4.方法：<a class="headerlink" href="#4" title="Permanent link">&para;</a></h3>
<p>在这一部分中，我们首先介绍了图对抗性训练的公式，然后介绍了 <em>Graph-VAT</em>，这是GraphAT的扩展，它进一步结合了虚拟对立我的正则化[35]。 然后，我们提出了两种节点分类任务的解决方案，即GraphAT和Graph-VAT，它们分别使用GraphAT和Graph-VAT来训练GCN[7]。 最后，我们分析 这两种解决方案的时间复杂性，并给出了重要的实现细节。</p>
<h4 id="41-gat-graph-adversarial-training">4.1 GAT （<strong>Graph Adversarial Training</strong>）<a class="headerlink" href="#41-gat-graph-adversarial-training" title="Permanent link">&para;</a></h4>
<p>对抗性训练的最新进展已经成功地学习了基于深度神经网络的分类器，使它们对广泛的标准分类tas的扰动具有鲁棒性 视觉识别[15]、[16]、[35]和文本分类[17]等。 一般情况下，应用AT可以调节模型参数来平滑输出分布[35]。 具体来说，每个clean example在数据集中的精益示例中，AT鼓励模型将类似的输出分配给从干净示例的人工输入(即对抗性示例。 受sta哲学的启发 在NDARDAT中，我们开发了图形对抗性训练，它以生成对抗性示例和优化ADVE上的附加正则化项的方式训练图形神经网络模块疟疾例子，以防止扰动的不利影响。 <strong>这里的重点是防止通过节点连接传播的扰动（如图1所示），或者对抗性训练中的图形结构。</strong></p>
<p><img alt="image-20200721095707729" src="E:\typora图片\095708-79832.png" /></p>
<p>Fig 2：GraphAT的训练过程：1)构造图对抗性实例；2)通过最小化损耗值和图对抗性正则化来更新模型参数。</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 这个图是怎么体现这个过程的？</p>
<p>GraphAT 的公式：</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\left.\min : \Gamma_{G A T}=\Gamma+\beta \sum_{i=1} \sum_{j \in \mathcal{N}_{i}} d\left(f\left(\boldsymbol{x}_{i}+\boldsymbol{r}_{i}^{g}, G \mid \boldsymbol{\Theta}\right), f\left(\boldsymbol{x}_{j}, G \mid \Theta\right)\right)\right) \\
\max : \boldsymbol{r}_{i}^{g}=\arg \max _{\boldsymbol{r}_{i},\left\|\boldsymbol{r}_{i}\right\| \leq \epsilon} \sum_{j \in \mathcal{N}_{i}} d\left(f\left(\boldsymbol{x}_{i}+\boldsymbol{r}_{i}, G \mid \hat{\Theta}\right), f\left(\boldsymbol{x}_{j}, G \mid \hat{\Theta}\right)\right)
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\left.\min : \Gamma_{G A T}=\Gamma+\beta \sum_{i=1} \sum_{j \in \mathcal{N}_{i}} d\left(f\left(\boldsymbol{x}_{i}+\boldsymbol{r}_{i}^{g}, G \mid \boldsymbol{\Theta}\right), f\left(\boldsymbol{x}_{j}, G \mid \Theta\right)\right)\right) \\
\max : \boldsymbol{r}_{i}^{g}=\arg \max _{\boldsymbol{r}_{i},\left\|\boldsymbol{r}_{i}\right\| \leq \epsilon} \sum_{j \in \mathcal{N}_{i}} d\left(f\left(\boldsymbol{x}_{i}+\boldsymbol{r}_{i}, G \mid \hat{\Theta}\right), f\left(\boldsymbol{x}_{j}, G \mid \hat{\Theta}\right)\right)
\end{array}
</script>
</div>
<p><span><span class="MathJax_Preview">\Gamma_{G A T}</span><script type="math/tex">\Gamma_{G A T}</script></span>训练目标函数有两个术语：基于原点的图形学习模型的标准目标函数（例如，方程(1)）和图形对抗性正则化。</p>
<p>第二项max公式鼓励将图对抗性示例类似地归类为连接示例，其中Θ表示要学习的参数，d是**度量的非负函数 两个预测之间的差异**(例如Kullback-Leibler散度[45]。$ \boldsymbol{r}_{i}^{g}$表示图对抗性扰动，它被用于clean exapmle i，来构造输入特征 一个图形对抗性的例子。</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> d()函数是什么意思，怎么度量？</p>
<p><strong>在模型参数的当前值下，通过最大化图对抗性正则化来计算图对抗性扰动</strong>。 也就是说，<strong>图的对抗性扰动是输入特征上的变化方向，它可以最大限度地攻击图形对抗性正则化</strong>，即从邻居节点传播的扰动的最坏情况。 是一个超参数控制 令扰动的大小，这通常被设置为小值，以便对抗性示例的特征分布接近干净示例。</p>
<p>一般来说，类似于标准AT，GraphAT的每一次迭代也可以看作是玩一个极小极大的游戏：· Maximization: 图AT从干净的例子w生成图的对抗性扰动最大限度地破坏了连接节点之间的平滑性。 然后通过将扰动添加到相关的干净示例的输入中来构造图形对抗性示例。</p>
<p>Minimization: 图AT通过鼓励对抗性预测之间的平滑性，在图对抗性示例上增加一个正则化器，使图神经网络的目标函数最小化 示例和连接示例。 因此，该模型对通过图传播的扰动变得鲁棒。</p>
<p>图2说明了图AT的过程。 而传统的基于图的正则化(例如，图Laplacian术语)也鼓励了图结构上预测的平滑性，图AT被认为是一个更先进的规则，原因有两个：1)图AT执行的正则化是动态的 由于对抗性示例是根据模型的当前参数和预测自适应生成的，而基于标准图形的正则化是静态的；2)图A Tto 在一定程度上增加了训练数据，因为在训练数据中没有出现生成的对抗性例子，这有利于模型的泛化。</p>
<p>近似: 得到$ \boldsymbol{r}_{i}^{g}$的封闭形式的解不平凡的。 在[15]提出的标准对抗性训练的线性逼近方法的启发下，我们还设计了一个线性应用程序 计算Graph-AT中图对抗性扰动的近似方法，其公式为：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{r}_{i}^{g} \approx \epsilon \frac{\boldsymbol{g}}{\|\boldsymbol{g}\|}, \text { where } \boldsymbol{g}=\nabla_{\boldsymbol{x}_{i}} \sum_{j \in \mathcal{N}_{i}} D\left(f\left(\boldsymbol{x}_{i}, G \mid \hat{\boldsymbol{\Theta}}\right), f\left(\boldsymbol{x}_{j}, G \mid \hat{\boldsymbol{\Theta}}\right)\right)
</div>
<script type="math/tex; mode=display">
\boldsymbol{r}_{i}^{g} \approx \epsilon \frac{\boldsymbol{g}}{\|\boldsymbol{g}\|}, \text { where } \boldsymbol{g}=\nabla_{\boldsymbol{x}_{i}} \sum_{j \in \mathcal{N}_{i}} D\left(f\left(\boldsymbol{x}_{i}, G \mid \hat{\boldsymbol{\Theta}}\right), f\left(\boldsymbol{x}_{j}, G \mid \hat{\boldsymbol{\Theta}}\right)\right)
</script>
</div>
<p>需要搞清楚：</p>
<p>$ \boldsymbol{r}_{i}^{g}$:是一个扰动</p>
<p>这个公式的来源：</p>
<p>FGSM和FGM方法是Goodfellow等人分别在[1]和[2]中提出的。思想很简单，就是让扰动<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>的方向是沿着梯度提升的方向的，沿着梯度提升也就意味着让损失增大的最大。FGSM（Fast Gradient Sign Method）和FGM（Fast Gradient Method）的区别在于采用的归一化的方法不同，FGSM是通过Sign函数对梯度采取max归一化，FGM则采用的是  L2归一化。max归一化是是说如果梯度某个维度上的值为正，则设为1；如果为负，则设为-1；如果为0，则设为0。<img alt="[公式]" src="E:\typora图片\equation-1596336791531.svg" />归一化则将梯度的每个维度的值除以梯度的<img alt="[公式]" src="https://www.zhihu.com/equation?tex=L_%7B2%7D" />范数。 理论上<img alt="[公式]" src="https://www.zhihu.com/equation?tex=L_%7B2%7D" />归一化更严格的保留了梯度的方向，但是max归一化则不一定和原始梯度的方向相同。</p>
<p>FGSM：<span><span class="MathJax_Preview">\delta=\epsilon \cdot \operatorname{sign}(g)</span><script type="math/tex">\delta=\epsilon \cdot \operatorname{sign}(g)</script></span></p>
<p>FGM: <img alt="[公式]" src="E:\typora图片\equation-1595564058679.svg" /></p>
<p>其中， <span><span class="MathJax_Preview">g=\nabla_{X}\left(L\left(f_{\theta}(X), y\right)\right)</span><script type="math/tex">g=\nabla_{X}\left(L\left(f_{\theta}(X), y\right)\right)</script></span> ，也就是损失函数L关于输入X的梯度，这个梯度在我们做神经网络优化的时候是很容易求出来的。</p>
<p>当然两种方法都有个假设，就是损失函数是线性的或者至少是局部线性的。如果不是（局部）线性的，那梯度提升的方向就不一定是最优方向了。</p>
<p><a href="https://zhuanlan.zhihu.com/p/104040055">https://zhuanlan.zhihu.com/p/104040055</a></p>
<h4 id="42-virtual-graph-adversarial-training">4.2 Virtual Graph Adversarial Training**<a class="headerlink" href="#42-virtual-graph-adversarial-training" title="Permanent link">&para;</a></h4>
<p>暂且不看</p>
<p>考虑到节点分类本质上是半监督学习的一项任务，我们进一步设计了一个扩展版本的GraphAT，名为GraphVAT，它进一步平滑了Pr的分布 围绕每个干净的例子，进一步增强模型的鲁棒性。 受虚拟对抗训练[35]思想的启发，我们进一步在traini中添加了一个虚拟对抗正则化器 吴目标函数并构造虚拟对抗性例子来攻击预测的局部平滑性。 与标准AT相比，它只考虑标记的干净示例，虚拟对抗性 培训还鼓励模型围绕未标记的清洁示例进行一致的预测。 Graph-VAT的公式为：</p>
<p><img src="E:\typora图片\111141-333486-1595316027899.png" alt="img" style="zoom: 80%;" /></p>
<h4 id="43-gcn">4.3  GCN<a class="headerlink" href="#43-gcn" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">
\sum_{i=1}^{M} \operatorname{cross-entropy}\left(f\left(\boldsymbol{x}_{i}, G \mid \boldsymbol{\Theta}\right), \boldsymbol{y}_{i}\right)+\lambda\|\boldsymbol{\Theta}\|_{F}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{M} \operatorname{cross-entropy}\left(f\left(\boldsymbol{x}_{i}, G \mid \boldsymbol{\Theta}\right), \boldsymbol{y}_{i}\right)+\lambda\|\boldsymbol{\Theta}\|_{F}^{2}
</script>
</div>
<p>这里那个正则化与公式（1）不相同，他通过引入了对抗训练中的对抗样本对训练进行干扰，形成的对抗性正则化，目的是增加模型的鲁棒性。</p>
<h2 id="300-paper">300-paper<a class="headerlink" href="#300-paper" title="Permanent link">&para;</a></h2>
<h3 id="1_1"><strong>1 引入：</strong><a class="headerlink" href="#1_1" title="Permanent link">&para;</a></h3>
<blockquote>
<p><img alt="🅰" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f170.png" title=":a:" />主要内容：</p>
<p><strong>对抗性训练可以作为一种正则化方法来提高泛化性能以及防止深度学习模型过度拟合。</strong></p>
<p>首先作者引入APR， 通过在嵌入空间中添加扰动，将对敌训练扩展到推荐系统中 朝着能够最大化损失函数的方向,但是 对抗性训练是[12]提出的一种新的方法，以解决深度学习模型很容易被对抗性例子[26]愚弄的问题，这些例子是通过施加小的代价来构造的 输入示例上的涡轮。 最近，一些研究[12,22,23]指出，<strong>对抗性训练可以作为一种正则化方法来提高泛化性能以及防止深度学习模型过度拟合。</strong> 最大方向扰动可能会移动嵌入示例x向量接近具有不同标签（不同示例**）甚至不存在示例的示例，不能保持原来的语义信息 在隐藏在用户-项目交互中，并没有充分利用对抗性训练对推荐系统的优势。**。如图一所示。因此，作者改进算法，提出DAT，能够扰动方向施加适当的限制来解决这个问题。</p>
</blockquote>
<h5 id="apr"><strong>APR与最大方向的介绍</strong><a class="headerlink" href="#apr" title="Permanent link">&para;</a></h5>
<p>虽然对抗性训练在计算机视觉(CV)领域取得了很大的成功，但很难直接将其应用于推荐系统。 这是因为输入数据是离散的和主要由高维单热向量（例如，对于用户和项）表示，它们不同于图像域中的连续值。 直接向离散值添加噪声是不合理的，因为它将改变输入示例的原始语义。 相反，最近的一项研究，APR， 通过在嵌入空间中添加扰动，将对敌训练扩展到推荐系统中 朝着能够最大化损失函数的方向。 为了简单起见，我们将这种扰动策略称为**最大方向**（maximum direction）。</p>
<p><img src="https://gitee.com/moluggg/image/raw/master/img/20200710203732.png"/></p>
<p>​                                        图一： 直观素描说明定向对抗性训练的过程。 </p>
<p><strong>观察到的与用户有交互作用的项目在小圆圈（白色圈）中，而在大圆圈中的其他项目是不可观察的 d项目（黄色圈）。圈外无意义</strong></p>
<p>对抗性训练的基本策略是输入 例x，它将保持相同的标签为x和对抗性的例子xadv扰动从x在训练期间。 这使得模型可以考虑x周围更多看不见的空间，从而鼓励loca 相似实例之间的l平滑性，进一步将模型推向更好的泛化[23]。 然而，如图1(A)所示，最大方向扰动可能会移动嵌入示例x向量接近具有不同标签（不同示例**）甚至不存在示例的示例**。 我们通过图2中的初步实验给出了一个详细的例子。 我们2(A)显示了ID16用户的原始Top-10最近邻，图2(B)显示了在嵌入空间中添加最大方向扰动后的Top-10最近邻。 作为我们可以看到，在最大方向扰动下，Top-10最近邻有很大的变化。 这意味着当**前的最大方向策略不能保持原来的语义信息 在隐藏在用户-项目交互中，并没有充分利用对抗性训练对推荐系统的优势。**</p>
<p><img src="https://gitee.com/moluggg/image/raw/master/img/20200710204549.png"/></p>
<h5 id="_2"><a class="headerlink" href="#_2" title="Permanent link">&para;</a></h5>
<blockquote>
<p>圆圈表示什么？观察到的与用户有交互作用的项目在小圆圈中，而在大圆圈中的其他项目是不可观察的 d项目。数字又是什么？表示邻居的编号</p>
</blockquote>
<h5 id="dat">DAT的引入以及扰动策略<a class="headerlink" href="#dat" title="Permanent link">&para;</a></h5>
<p>在本文中，我们通过**对扰动方向施加适当的限制**来解决这个问题，我们称之为定向对抗性训练(DAT)。 我们将一个例子x扰动到嵌入空间中的另一个x' ,一个权重w被设置为控制x向x0的距离， DAT引入了一个额外的对抗性损失，使得训练过程发挥了极小极大的游戏：</p>
<p>权重w将通过**最大化**贝叶斯个性化排名(B PR)损失[24]来计算；接下来，对模型进行训练，以最小化B PR损失和DAT损失。 此外，我们考虑产生更多的效果 在协同滤波(CF)推荐系统中，通过引导扰动方向与关键协作信号进行有效嵌入。 直觉是具**有相似行为的用户w 我们对物品表现出类似的偏好**。</p>
<p>图1(B)说明了我们的方向扰动策略。 给定一个用户u，我们根据预先训练的模型参数，使它对嵌入空间中的Top-K最近邻进行扰动。 意思是**在小圆中，观察到的项目I和未观察到的项目j的扰动方向将被扰动到小圆中的其他观察项目和大圆中的未观察项目**，。 这样，**类似例子的信息就会在彼此之间流动，**这明确地将协作信号注入到对抗性学习过程中。 图2(C)显示了前10名 在添加我们的方向扰动后，用户16的最近邻居，这与原来的方向扰动更相干。</p>
<h5 id="_3">本文的主要贡献总结：<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h5>
<p>我们通过初步实验研究了现有对抗性训练方法在推荐中的局限性。 </p>
<p>我们支持 提出了一种新的技术，定向对抗性训练(DAT)，通过限制扰动方向，并将协作信号显式编码到对抗性学习过程中。 我们 演示了DAT在广义矩阵因式分解(GM F)[15]上的应用，从而得到了我们的方法-定向对抗性训练广义矩阵化(DAGMF)。 </p>
<p>我们进行实验 在三个公共数据集上验证DAT对推荐系统的有效性。 具体来说，DAGMF在HR（命中率）和NDCG(归一化折扣Cu)上都有显著的改善 多项增益)指标与最先进的模型相比。</p>
<h3 id="2_1">2 相关工作<a class="headerlink" href="#2_1" title="Permanent link">&para;</a></h3>
<p><strong>建议</strong>。 随着信息的蓬勃发展，用户找到符合他们偏好的项目是一个巨大的挑战。 CF技术通过假设用户相似来解决这个挑战 行为对项目表现出类似的偏好，并专注于开发用户-项目交互。 基于邻域的CF[21]是利用显式相似的方法，是早期有效的CF方法之一测量（例如欧氏距离、余弦相似度）来计算用户和项目之间的交互强度。 基于模型的CF是最流行和最广泛使用的推荐方法之一近年来的ES。 矩阵因式分解(M F)[19]方法是基于模型的CF方法的重要实现。 它根据原始用户项比率的分解来预测未知的评级矩阵 通过将每个用户和项目的一个热向量映射为嵌入向量，然后在它们之间进行内积。 然而**，MF的线性结构使它无法捕获复杂和非用户与项目[15]之间的线性交互。 为此**，最近的一些工作[15,6]将深度学习技术应用于CF推荐系统，如GMF。 这是通过建模用户项之间完成的具有多层感知器(MLP)的动作，可以学习更强大和更有表现力的交互功能。 后来，为了学习更有效的嵌入，已经投入了大量的精力来整合 侧信息，如文本或图像内容特征[5]，邻居关系[29]，用户和项目的属性[20,31]以及协作信号[30,7]。 此外，基于届会的建议[13]建议在模拟用户偏好时考虑基于会话的数据，而不是CF方法。 </p>
<p><strong>对抗性培训</strong>。 一开始，提出对抗性训练来解决最先进的分类模型由于其线性结构[12]而容易受到对抗性例子的影响。 它用可以生成的对抗性例子来训练模型 采用快速梯度符号法[12]有效地提高了模型的鲁棒性。 后来，对抗性训练的思想也被推广到自然语言上处理(NLP)任务（例如序列标签）[22]和网络分类器任务[11,8]，其中扰动被添加到嵌入而不是输入上。 他们证明了他们的策略 更可以看作是提高分类器模型泛**化性能的正则化方法，而不是作为防御方法工作**。 值得注意的是，大多数现有的作品都是关于阿巴拉里的所有的训练都集中在CV领域。 很少有研究来探索推荐系统的对抗性训练。 [28]提出了推荐系统中的极小极大极小博弈，即基于生成的IRGAN 对抗网(GANs)框架，这是为了学习更有效的嵌入MF。 [4]是基于向量GAN的CF，而不学习嵌入向量。 他们有非常复杂的框架工作和遭受公认的艰苦训练问题。 APR[14]考虑利用推荐系统中的对抗性训练作为增加最大扰动的正则化 在嵌入空间中忽略协作信号。</p>
<p>值得注意的是，现有的对抗性培训大多集中在cv 领域。针对推荐系统的对抗训练的研究很少。[28]提出了推荐系统中的极小极大对策，即IRGAN
生成对抗网(GANs)框架，这是学习更有效的嵌入MF。CFGAN[4]是矢量方向的</p>
<p>基于gan的CF，不需要学习嵌入向量。他们有非常复杂的框架和遭受公认的艰苦训练问题。APR[14]考虑在高校开展对抗性训练</p>
<p>DAT应用于推荐系统中</p>
<h3 id="3">3 初步<a class="headerlink" href="#3" title="Permanent link">&para;</a></h3>
<blockquote>
<p><img alt="🅰" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f170.png" title=":a:" /> 我们首先介绍了推荐系统的技术背景和**GMF**的制定。</p>
<p>然后，我们重新描述了**贝叶斯个性化排序**方法，并给出了一个成对排序损失函数。</p>
</blockquote>
<h4 id="31">3.1 问题定义<a class="headerlink" href="#31" title="Permanent link">&para;</a></h4>
<p>形式上，我们表示有一组m用户，U={u1，u2，...，um}，一组n项，i={i1，i1，...，in}和稀疏矩阵R×n。 如果用户u有i，则表示的R中的条目(u，i)为：若u,i之间有相互作用则表示为为<span><span class="MathJax_Preview">R_{ui}</span><script type="math/tex">R_{ui}</script></span>=1。 </p>
<p>在基于模型的推荐系统中，<strong>输入由两个特征向量<span><span class="MathJax_Preview">v_{u}^{U}</span><script type="math/tex">v_{u}^{U}</script></span>和<span><span class="MathJax_Preview">v_{i}^{I}</span><script type="math/tex">v_{i}^{I}</script></span>组成，分别描述用户u和项目i</strong>。 </p>
<p>在本文中**，输入向量是一个二进制稀疏向量，通过一个one-hot编码用户或项目的身份。**</p>
<p>在输入的上方是嵌入层，它将 one-hot **稀疏特征向量----&gt;密集的embbding**向量。 之后，用户或项目将由密集向量表示，也称为嵌入向量（embedding vector)。</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 如何从onehot编码转变成密集的embbeding？</p>
<p>接下来，用户和项嵌入向量将被输入到交互函数中 去 预测分数的潜在向量。 然后让rcui表示用户u在i项上的预测偏好得分。 推荐系统中rcui的计算制定如下:</p>
<div>
<div class="MathJax_Preview">
\widehat{r_{u i}}=f\left(\mathbf{p}_{u}, \mathbf{q}_{i} \mid \mathbf{P}, \mathbf{Q}, \Theta_{f}\right)\tag{1}
</div>
<script type="math/tex; mode=display">
\widehat{r_{u i}}=f\left(\mathbf{p}_{u}, \mathbf{q}_{i} \mid \mathbf{P}, \mathbf{Q}, \Theta_{f}\right)\tag{1}
</script>
</div>
<p>注： </p>
<p>①f（·）是交互函数( the interaction function)，建模用户对项目I的偏好，Θf是它的参数。 </p>
<p>②我们定义<span><span class="MathJax_Preview">P=\{p_u\}_{u∈U}</span><script type="math/tex">P=\{p_u\}_{u∈U}</script></span>表示用户的嵌入矩阵，<span><span class="MathJax_Preview">Q=\{q_i\}_{i∈I}</span><script type="math/tex">Q=\{q_i\}_{i∈I}</script></span>表示这是项目的嵌入矩阵。 </p>
<p>③设<span><span class="MathJax_Preview">p_u=P_tv^U_u(p_u∈R^D)</span><script type="math/tex">p_u=P_tv^U_u(p_u∈R^D)</script></span>和<span><span class="MathJax_Preview">q_u=Q_tv^I_i(q_u∈R^D)</span><script type="math/tex">q_u=Q_tv^I_i(q_u∈R^D)</script></span>，分别表示用户u和i项的嵌入向量，D 为嵌入向量的维数</p>
<p>④ <strong>输入由两个特征向量<span><span class="MathJax_Preview">v_{u}^{U}</span><script type="math/tex">v_{u}^{U}</script></span>和<span><span class="MathJax_Preview">v_{i}^{I}</span><script type="math/tex">v_{i}^{I}</script></span>组成，分别描述用户u和项目i</strong></p>
<p>​    NCF[15]是一种深入的CF框架，它是为了捕捉用户与具有多层感知器(MLP)的项目之间复杂而非线性的关系而提出的)。 <strong>GMF是最具代表性的合作伙伴之一 在此框架下构造了滤波模型。 在GMF中，利用MLP和内积将两两用户嵌入和项嵌入映射到偏好分数。</strong></p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 这是GMF的表示吧</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\phi_{1}\left(\mathbf{p}_{u}, \mathbf{q}_{i}\right)=\mathbf{p}_{u} \odot \mathbf{q}_{i} \\
\widehat{r_{u i}}=\phi_{o u t}\left(\phi_{x} \ldots\left(\phi_{2}\left(\phi_{1}\left(\mathbf{p}_{u}, \mathbf{q}_{i}\right)\right)\right)\right)
\end{array}\tag{2}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\phi_{1}\left(\mathbf{p}_{u}, \mathbf{q}_{i}\right)=\mathbf{p}_{u} \odot \mathbf{q}_{i} \\
\widehat{r_{u i}}=\phi_{o u t}\left(\phi_{x} \ldots\left(\phi_{2}\left(\phi_{1}\left(\mathbf{p}_{u}, \mathbf{q}_{i}\right)\right)\right)\right)
\end{array}\tag{2}
</script>
</div>
<p>注：该公式是GMF算法</p>
<p>① 其中φ1是用户与项目嵌入向量之间的内积，</p>
<p>② ⊙一般是同或符号，相同为1，但是这里的意思是向量或者矩阵的**内积**</p>
<p>③ φx和φout是GMF的第x层和输出MLP层。（GMF算法得到的偏好分数）</p>
<h4 id="32-bpr">3.2 引入BPR：<a class="headerlink" href="#32-bpr" title="Permanent link">&para;</a></h4>
<div>
<div class="MathJax_Preview">
L_{B P R}=\sum_{(u, i, j) \in T}-\ln \sigma(\widehat{r_{u i}}-\widehat{r_{u j}})+\lambda\|\Theta\|^{2}\tag{3}
</div>
<script type="math/tex; mode=display">
L_{B P R}=\sum_{(u, i, j) \in T}-\ln \sigma(\widehat{r_{u i}}-\widehat{r_{u j}})+\lambda\|\Theta\|^{2}\tag{3}
</script>
</div>
<p>注：学过</p>
<h3 id="4_1">4 措施：<a class="headerlink" href="#4_1" title="Permanent link">&para;</a></h3>
<blockquote>
<p><img alt="🅰" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f170.png" title=":a:" />在这一部分中，我们首先介绍了**对抗性训练GMF**(AGMF)的体系结构)。 接下来，作者**介绍了DAGMF**方法，这是DAT与GMF相结合的实例化。 最后，选择的策略 详细讨论了扰动的方向。</p>
</blockquote>
<h4 id="41-bpr">4.1 构造对抗正则项并应用到BPR上：<a class="headerlink" href="#41-bpr" title="Permanent link">&para;</a></h4>
<div class="codehilite"><pre><span></span><code>对抗性训练是[12]提出的一种新的正则化方法，以提高分类器模型在CV域中的鲁棒性。 与CV领域不同，[22]在CV领域应用对抗性训练 NLP域不再增加输入空间的扰动。 他们在嵌入层中加入对抗性扰动，并认为这种方法是有效的正则化，以防止过拟合提高泛化性能。 与语言处理类似，**APR[14]通过干扰表示用户和项目的嵌入向量，引入MF-BPR中的对抗性训练** Ms。 在本文中，我们希望在深度学习模型GMF中扩展APR，**只在嵌入层即AGMF中施加扰动。**
</code></pre></div>


<p>​   在这一部分中，步骤为：</p>
<p>对抗性训练GMF(AGMF)的体系结构)-------&gt;我们介绍了DAGMF方法(这是DAT与GMF相结合的实例化) --------&gt;  选择的策略 详细讨论了扰动的方向。</p>
<p>现在可以计算**第i项中用户u的预测偏好分数**：</p>
<div>
<div class="MathJax_Preview">
\widehat{r_{u i \Delta}}=f\left(\mathbf{p} \Delta_{a d v}^{u}, \mathbf{q} \Delta_{a d v}^{i} \mid \mathbf{P}_{\Delta a d v}, \mathbf{Q}_{\Delta a d v}, \Theta_{f}\right)\tag{4}
</div>
<script type="math/tex; mode=display">
\widehat{r_{u i \Delta}}=f\left(\mathbf{p} \Delta_{a d v}^{u}, \mathbf{q} \Delta_{a d v}^{i} \mid \mathbf{P}_{\Delta a d v}, \mathbf{Q}_{\Delta a d v}, \Theta_{f}\right)\tag{4}
</script>
</div>
<p>注：</p>
<p>① <span><span class="MathJax_Preview">\mathbf{P}_{\boldsymbol{\Delta} a d v}=\left\{\mathbf{p} \Delta_{a d v}^{u}\right\}_{u \in U}</span><script type="math/tex">\mathbf{P}_{\boldsymbol{\Delta} a d v}=\left\{\mathbf{p} \Delta_{a d v}^{u}\right\}_{u \in U}</script></span>  表示用户的对抗性嵌入矩阵,</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" />是其中一个，不是所有的用户吧</p>
<p>② <span><span class="MathJax_Preview">\mathbf{Q}_{\boldsymbol{\Delta} a d v}=\left\{\mathbf{q} \Delta_{a d v}^{u}\right\}_{i \in I}</span><script type="math/tex">\mathbf{Q}_{\boldsymbol{\Delta} a d v}=\left\{\mathbf{q} \Delta_{a d v}^{u}\right\}_{i \in I}</script></span>  同上</p>
<p>③ <span><span class="MathJax_Preview">p ∆^u_{adv} =  P_u + ∆^u_{adv}</span><script type="math/tex">p ∆^u_{adv} =  P_u + ∆^u_{adv}</script></span>  表 示用户u的对抗性嵌入向量， <span><span class="MathJax_Preview">∆^u_{adv}</span><script type="math/tex">∆^u_{adv}</script></span> 表示要添加到用户u的嵌入向量中的对抗性扰动。 <span><span class="MathJax_Preview">∆^u_{adv}</span><script type="math/tex">∆^u_{adv}</script></span>  的维数大小为D，与用户u嵌入向量相同</p>
<p>④ 由三可以得知<span><span class="MathJax_Preview">q ∆^i_{adv}</span><script type="math/tex">q ∆^i_{adv}</script></span>的来由</p>
<p><strong>对抗性扰动旨在最大化推荐模型的目标函数</strong>。 因此，我们将其定义如下：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\Delta}_{a d v}=\underset{\Delta,\|\Delta\| \leq \epsilon}{\arg \max } L_{B P R}\left(T \mid \widehat{\boldsymbol{\Theta}}+\boldsymbol{\Delta}, \Theta_{f}\right)\tag{5}
</div>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}_{a d v}=\underset{\Delta,\|\Delta\| \leq \epsilon}{\arg \max } L_{B P R}\left(T \mid \widehat{\boldsymbol{\Theta}}+\boldsymbol{\Delta}, \Theta_{f}\right)\tag{5}
</script>
</div>
<p>①​ <img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 这里的<span><span class="MathJax_Preview">\widehat{\boldsymbol{\Theta}}</span><script type="math/tex">\widehat{\boldsymbol{\Theta}}</script></span>不是权重参数吧？</p>
<p>其中Θb是用户和项目的嵌入矩阵，包括P和Q，</p>
<p>② ∆表示嵌入矩阵的扰动，<span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>表示控制∆范数的超参数。</p>
<p>③  T表示所有成对训练实例的集。</p>
<p>④ <img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 的 <span><span class="MathJax_Preview">\boldsymbol{\Delta}_{a d v}</span><script type="math/tex">\boldsymbol{\Delta}_{a d v}</script></span> 和<span><span class="MathJax_Preview">∆^u_{adv}</span><script type="math/tex">∆^u_{adv}</script></span> 有何关系？</p>
<p>本文采用L2范数(k)。</p>
<p>在**上述方程中得到<span><span class="MathJax_Preview">∆_{adv}</span><script type="math/tex">∆_{adv}</script></span>_估计是很难的**。 因为目标函数和神经网络涉及复杂的操作。 因此，[12]提出了一种**快速梯度符号的逼近方法**，通过对∆周围的目标函数进行线性化。，表示如下：</p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\Delta}_{a d v}=\epsilon \frac{\mathbf{g}}{\|\mathbf{g}\|}, \mathbf{g}=\nabla_{\widehat{\Theta}} L_{B P R}\left(T \mid \widehat{\mathbf{\Theta}}+\Delta, \Theta_{f}\right)\tag{6}
</div>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}_{a d v}=\epsilon \frac{\mathbf{g}}{\|\mathbf{g}\|}, \mathbf{g}=\nabla_{\widehat{\Theta}} L_{B P R}\left(T \mid \widehat{\mathbf{\Theta}}+\Delta, \Theta_{f}\right)\tag{6}
</script>
</div>
<p>现在， 构造对抗正则项并应用到BPR上</p>
<div>
<div class="MathJax_Preview">
L_{B P R \Delta}=\sum_{(u, i, j) \in T}-\ln \sigma\left(\widehat{r_{u i}} _\Delta-\widehat{r_{u j}}_{\Delta}\right)\tag{7}
</div>
<script type="math/tex; mode=display">
L_{B P R \Delta}=\sum_{(u, i, j) \in T}-\ln \sigma\left(\widehat{r_{u i}} _\Delta-\widehat{r_{u j}}_{\Delta}\right)\tag{7}
</script>
</div>
<div>
<div class="MathJax_Preview">
L_{a d v}=L_{B P R}+\lambda L_{B P R \Delta}\tag{8}
</div>
<script type="math/tex; mode=display">
L_{a d v}=L_{B P R}+\lambda L_{B P R \Delta}\tag{8}
</script>
</div>
<p><img alt="image-20200801113549697" src="E:\typora图片\113551-650670.png" /></p>
<h4 id="42-dat">4.2 DAT<a class="headerlink" href="#42-dat" title="Permanent link">&para;</a></h4>
<p>然而，只有考虑到推荐模型嵌入空间中向最大方向的扰动，才能将一些噪声信息带入训练过程。 因此，我们建议DAT对抗性培训，以解决这一问题，以更好地应用对抗性培训在推荐系统。 我们方法的直觉**是嵌入空间中的扰动方向将限制在现有嵌入空间中的其他例子上，而不是最坏的扰动方向**，这样我们就可以将协作信号集成到训练过程中。 在游戏中 极小极小博弈，嵌入层的质量将逐步提高。 图3说明了在模型GMF中应用定向对抗性训练的框架。</p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> worst perturbation direction与maximum direction in</p>
<p>the embedding space 有何关系？</p>
<p>我们定义了从用户ut到用户UZ的嵌入空间中的方向向量：</p>
<div>
<div class="MathJax_Preview">
\mathbf{d}_{u_{z}}^{u_{t}}=\frac{\widetilde{\mathbf{d}}_{u_{z}}^{u_{t}}}{\left\|\tilde{\mathbf{d}}_{u_{z}}^{u_{t}}\right\|}, \tilde{\mathbf{d}}_{u_{z}}^{u_{t}}=\mathbf{p}_{u_{z}}-\mathbf{p}_{u_{t}}\tag{9}
</div>
<script type="math/tex; mode=display">
\mathbf{d}_{u_{z}}^{u_{t}}=\frac{\widetilde{\mathbf{d}}_{u_{z}}^{u_{t}}}{\left\|\tilde{\mathbf{d}}_{u_{z}}^{u_{t}}\right\|}, \tilde{\mathbf{d}}_{u_{z}}^{u_{t}}=\mathbf{p}_{u_{z}}-\mathbf{p}_{u_{t}}\tag{9}
</script>
</div>
<p>在我们的方法中，<strong>训练示例将扰动的用户被定义为目标集</strong></p>
<div>
<div class="MathJax_Preview">
\boldsymbol{\Delta}\left(w^{u}\right)=w_{u^{\prime}}^{u} \mathbf{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}^{\prime}}\tag{10}
</div>
<script type="math/tex; mode=display">
\boldsymbol{\Delta}\left(w^{u}\right)=w_{u^{\prime}}^{u} \mathbf{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}^{\prime}}\tag{10}
</script>
</div>
<p>注：</p>
<p>①  <span><span class="MathJax_Preview">{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}}</span><script type="math/tex">{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}}</script></span> 表示就是上述公式</p>
<p>②  我们将 $w_{u<sup>{\prime}}</sup>{u} $ 定义为对应于方向向量<span><span class="MathJax_Preview">{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}}</span><script type="math/tex">{d}_{\mathbf{u}^{\prime}}^{\mathbf{u}}</script></span>的权重</p>
<p>然后，将**用户u的定向对抗性嵌入向量表示**为：</p>
<div>
<div class="MathJax_Preview">
\mathbf{p}_{\Delta_{d a d v}^{u}}=\mathbf{p}_{u}+\Delta\left(w^{u}\right)\tag{11}
</div>
<script type="math/tex; mode=display">
\mathbf{p}_{\Delta_{d a d v}^{u}}=\mathbf{p}_{u}+\Delta\left(w^{u}\right)\tag{11}
</script>
</div>
<p>由于BPR损失将在训练过程中最小化，我们**考虑如何寻求方向向量的最坏情况权重**，从而使BPR损失最大化：</p>
<div>
<div class="MathJax_Preview">
w_{\text {dadv}}^{u}=\underset{w^{u},\left\|w^{u}\right\| \leq \epsilon}{\arg \max } L_{B P R}\left(T \mid \widehat{\Theta}+\Delta\left(w^{u}\right), \Theta_{f}\right)\tag{12}
</div>
<script type="math/tex; mode=display">
w_{\text {dadv}}^{u}=\underset{w^{u},\left\|w^{u}\right\| \leq \epsilon}{\arg \max } L_{B P R}\left(T \mid \widehat{\Theta}+\Delta\left(w^{u}\right), \Theta_{f}\right)\tag{12}
</script>
</div>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" />这里的<span><span class="MathJax_Preview">\widehat{\Theta}</span><script type="math/tex">\widehat{\Theta}</script></span> 就是上述同时中的P吧</p>
<p>上文公式（5）改善成公式（12），对应的与公式（10）相邻的几个函数也要改变。改变步骤如下：</p>
<p><strong>公式（6,7,8）----&gt;公式（13,14,15）</strong></p>
<p>在限制扰动方向后，在计算最坏情况权重时不再是公式11。 因此，我们借用[23]的想法，通过在LBPR(T|Θb∆(Wu)，Θf)中应用二阶泰勒，可以近似估计扰动强度Wu)。 形式上，估计最坏情况权重的解决方案可以定义为如下：</p>
<div>
<div class="MathJax_Preview">
w_{d a d v}^{u}=\epsilon \frac{g}{\|g\|}, g=\nabla_{w^{u}} L_{B P R}\left(T \mid \widehat{\Theta}+\Delta\left(w^{u}\right), \Theta_{f}\right)\tag{13}
</div>
<script type="math/tex; mode=display">
w_{d a d v}^{u}=\epsilon \frac{g}{\|g\|}, g=\nabla_{w^{u}} L_{B P R}\left(T \mid \widehat{\Theta}+\Delta\left(w^{u}\right), \Theta_{f}\right)\tag{13}
</script>
</div>
<p>类似地，则重新定义对抗性训练的总体目标函数优化如下：</p>
<div>
<div class="MathJax_Preview">
&lt;\widehat{\Theta}, \Theta_{f}&gt;=\underset{\hat{\Theta}, \Theta_{f}}{\arg \min } L_{B P R}+\lambda L_{B P R \Delta_{d a d v}}\tag{14}
</div>
<script type="math/tex; mode=display">
<\widehat{\Theta}, \Theta_{f}>=\underset{\hat{\Theta}, \Theta_{f}}{\arg \min } L_{B P R}+\lambda L_{B P R \Delta_{d a d v}}\tag{14}
</script>
</div>
<p>通过对上述两个过程的统一，我们可以制定一个极小极大目标函数。 模型参数的优化&lt;Θb，Θf&gt;是最小化玩家，并寻求最坏情况下的扰动 重量是最大的玩家：</p>
<div>
<div class="MathJax_Preview">
&lt;\widehat{\mathbf{\Theta}}, \Theta_{f}, w_{\text {dadv}}&gt;=\underset{\hat{\mathbf{\Theta}}, \Theta_{f}}{\arg \min } \underset{w_{\text {dadv}}}{\arg \max } L_{B P R}+\lambda L_{B P R \Delta_{\text {dadv}}}\tag{15}
</div>
<script type="math/tex; mode=display">
<\widehat{\mathbf{\Theta}}, \Theta_{f}, w_{\text {dadv}}>=\underset{\hat{\mathbf{\Theta}}, \Theta_{f}}{\arg \min } \underset{w_{\text {dadv}}}{\arg \max } L_{B P R}+\lambda L_{B P R \Delta_{\text {dadv}}}\tag{15}
</script>
</div>
<p>从另一个角度来看，我们的DAT可以看作是一种数据增强的方法，它同时在包含协作信号的原始数据和扰动数据上训练模型。 w由于本文的重点是对抗性训练，因此将这一探索作为未来的工作。 算法1显示了我们的DAGMF的详细训练过程。 最后，值得注意的是，在BE中轧钢，模型参数Θb，Θf是通过优化GMF（第1行）初始化的，而不是随机的。 这是因为当模型欠拟合时，正常的训练过程就足以得到更好的参数。 在模型参数开始过拟合数据后，需要添加对抗性扰动。</p>
<h2 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h2>
<p>作者在[2]是第一个定义训练神经概率语言模型使用Softmax交叉熵训练损失。 Softmax在计算上是缓慢的，快速的近似建议加快培训。 由作者在[4]引入的Softmax采样显示了时间复杂性和最先进的结果[3,12]的巨大改进。 采样Softmax避免计算 词汇中每个单词的ING分数，因为你选择一个提案分布，从中它是便宜的样本，并执行一个有偏见的重要性抽样近似的Softmax的梯度。 噪音还引入了对比估计[10]作为Softmax的**无偏估计**，并证明了它对学习单词嵌入[25]的有效性。 相反，[22,23]支柱的作者 一个负抽样损失，其目标不是近似Softmax，而是学习单词的高质量向量表示。 因此，他们训练一个分类器，可以区分 正对来自真实分布，假对来自负抽样分布。</p>
<p>什么是无偏估计？</p>
<p>前面提到的负采样方案的引入，以及[22,23]提出的Word2Vec体系结构，使得在质量方面取得了最先进的结果 学习单词嵌入。 Word2Vec已成为一种广泛使用的体系结构，在文档分类[15]、用户建模[34]和图形嵌入[9,26]等应用中得到了应用。 最近 工作导致了单词嵌入模型[6,28]的进一步改进；一个值得注意的例子是快速文本模型[13]，它利用n-gram特征来提高单词嵌入质量。</p>
<p>正和未标记(P U)学习最初是在[19]中定义的。 研究表明，将PU学习应用于语言建模和推荐可以提高性能。 作者在[21]是第一个将PU学习框架应用于文本文档分类的人，并且在准确性上表现出显著的提升。 对于单词嵌入的训练，[31]定义的作者 一个启发式的所有未观察到的对，<strong>并鼓励模型尊重一个成对依赖的上限在其PMI估计</strong>，利用更大的信息。 在协同过滤领域吴，[20]的作者结合了一个**用户暴露于项目的模型**（incorporate a model of user exposure to items），使得每个未观察到的对被不同的对待。 有趣的是，在上一小节中引入了负抽样方案可以看作是将问题转移到PU学习案例的一种方法。 与传统的通用抽样损失(如Sampled Softmax或噪声对比估计)相反，我们主张**负 抽样方案不应试图近似Softmax函数，而应作为一种方法来诱导对负模型的信息偏差**。 为此，作者在[7]提供了一个负抽样的有价值的分析表明，<strong>具有较高内积分数的负样本（high inner product scores）具有上下文词提供了更多的信息梯度</strong>。 PU学习方法已经显示了一些 成功的背景下的推荐系统，如[20]所示。 此外，正如以前的几篇作品[5,24,37]强调的那样**，语言建模是高度依赖上下文的，我们在本文中提倡 一种基于上下文的负抽样方案。**</p>
<p>1.3  一种基于主动学习的负抽样方案</p>
<p>在主动学习的情况下，模型访问了一个小的标记数据集和一个更大的未标记数据池。 然后，目标是查询其中最有价值数据点的标签广泛的未标记集。 因此，使用抽样方法，可以用明智选择的点扩展训练集，以减少模型（[30]）或方差所造成的**泛化误差**数据集（[38]）。 在主动学习中，一个著名的研究领域是**不确定性抽样**（[11,32]）。 在此设置中，<strong>查询最不确定的实例以帮助模型细化决策 边界</strong>。 相反，在最大模型变化抽样中，一个是寻找具有最高范数梯度的样本点来提高性能（[1,14]）。 我们请感兴趣的读者[36]f 或者更详尽地比较主动学习方法。 在我们的例子中，我们永远无法访问采样点的真实标签，但我们认为理想的负采样应该是a 接近决策边界的采样数据点与采样非常不可能的数据之间的相对权衡。</p>
<p>1.负采样</p>
<p>自然语言处理领域中，判断两个单词是不是一对上下文词（context）与目标词（target），如果是一对，则是正样本，如果不是一对，则是负样本。</p>
<ol>
<li>
<p>泛化</p>
</li>
<li>
<p>泛化误差</p>
</li>
</ol>
<p>学习方法的泛化能力（Generalization Error）是由该方法学习到的模型对未知数据的预测能力。</p>
<p>如果学得到的模型是<span><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span></p>
<p>，那么用这个模型对未知数据预测的误差即为泛化误差</p>
<p><span><span class="MathJax_Preview">R_{\exp }(\hat{f})=E_{P}[L(Y, \hat{f}(X))]=\int_{x \times y} L(y, \hat{f}(x)) P(x, y) d x d y</span><script type="math/tex">R_{\exp }(\hat{f})=E_{P}[L(Y, \hat{f}(X))]=\int_{x \times y} L(y, \hat{f}(x)) P(x, y) d x d y</script></span></p>
<ol>
<li>不确定性抽样</li>
</ol>
<p><a href="https://blog.csdn.net/qq_39856931/article/details/106433187">https://blog.csdn.net/qq_39856931/article/details/106433187</a></p>
<ol>
<li>主动学习</li>
</ol>
<p><strong>主动学习</strong>(Active Learning)为我们提供了这种可能。主动学习通过一定的算法查询**最有用的**未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。</p>
<p>因此，为了尽可能地减小训练集及标注成本，在机器学习领域中，提出主动学习（active learning）方法，优化分类模型。</p>
<p>最有用的样本是什么？</p>
<p>在各种主动学习方法中，查询函数的设计最常用的策略是：**不确定性**准则（uncertainty）和**差异性**准则（diversity）。</p>
<p>对于不确定性，我们可以借助信息熵的概念来进行理解。我们知道信息熵是衡量信息量的概念，也是衡量不确定性的概念。信息熵越大，就代表不确定性越大，包含的信息量也就越丰富。事实上，有些基于不确定性的主动学习查询函数就是使用了信息熵来设计的，比如熵值装袋查询（Entropy query-by-bagging）。所以，不确定性策略就是要想方设法地找出不确定性高的样本，因为这些样本所包含的丰富信息量，对我们训练模型来说就是有用的。</p>
<p>那么差异性怎么来理解呢？之前说到或查询函数每次迭代中查询一个或者**一批**样本。我们当然希望所查询的样本提供的信息是全面的，各个样本提供的信息不重复不冗余，即样本之间具有一定的差异性。在每轮迭代抽取单个信息量最大的样本加入训练集的情况下，每一轮迭代中模型都被重新训练，以新获得的知识去参与对样本不确定性的评估可以有效地避免数据冗余。但是如果每次迭代查询一批样本，那么就应该想办法来保证样本的差异性，避免数据冗余。</p>
<p>主动学习与半监督学习的区别：</p>
<p>主动学习，在利用未标注数据的时候，是从未标注数据中找到最容易判断错误的样例来交由专家进行标注，这个过程是一个筛选差数据的过程，也是一个互动交互的过程，引入了额外的专家的知识。
半监督学习，尤其是对于自学习模型，对于未标注数据而言，是选择最不容易判断错误的样例来加入到已标注数据中，这个过程，是一个自动的过程，是筛选最好的数据的过程，然后不需要互动，不需要人工干预，基于自身对于未标记数据加以利用，来提高学习模型的泛化性能</p>
<ol>
<li><a href="https://www.zhihu.com/question/22983179">无偏估计与有偏估计</a></li>
</ol>
<p><a href="https://www.matongxue.com/madocs/607.html">为什么样本方差（sample variance）的分母是 n-1？</a></p>
<p>采样，并估计女生身高：
$$
\begin{array}{c}
\left{x_{1}, x_{2}, \cdots, x_{n}\right} \
\bar{X}=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}
\end{array}
$$
多次采样，平均值总会在一个 扰动：</p>
<p><img alt="img" src="https://gitee.com/moluggg/image/raw/master/img/202008/03/115855-153802.jpeg" /></p>
<p><a href="https://pic2.zhimg.com/50/v2-e3b97b0cde80871ce13909371341ab94_hd.jpg?source=1940ef5c">https://pic2.zhimg.com/50/v2-e3b97b0cde80871ce13909371341ab94_hd.jpg?source=1940ef5c</a></p>
<p>那么整体的方差是：
$$
S^{2}=\frac{1}{n} \sum_{i=1}<sup>{n}\left(X_{i}-\bar{X}\right)</sup>{2}
$$</p>
<h2 id="fgsm">FGSM<a class="headerlink" href="#fgsm" title="Permanent link">&para;</a></h2>
<div>
<div class="MathJax_Preview">
\left\{\begin{array}{c}
w^{T} \tilde{x}=w^{T} x+w^{T} \eta \\
\|\eta\|_{\infty}&lt;\epsilon \\
\operatorname{dim}(w)=n \\
\text { average }(G(x))=m
\end{array}\right.
</div>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}
w^{T} \tilde{x}=w^{T} x+w^{T} \eta \\
\|\eta\|_{\infty}<\epsilon \\
\operatorname{dim}(w)=n \\
\text { average }(G(x))=m
\end{array}\right.
</script>
</div>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" />对抗扰动通过ωTη影响激活函数从而影响分类结果。如果ω有n个维度，权向量的一个元素的平均大小是m，那么激活函数就会增加εmn。虽然‖η‖∞不会随着维度增加而增加，但是η会随n线性增长，然后就有高维问题了。</p>
<p>非线性模型的线性扰动
我们假设神经网络都太线性了以至于不能抵抗对抗样本。常见的LSTM、ReLU和maxout网络都趋向于线性表现，而类似sigmod这种非线性性模型也把大量的时间花在非饱和和线性系统中。（这里我也不怎么理解，望大神讲解）</p>
<p>我们从公式开始理解（fast gradient sign method）：
$η=εsign(▽xJ(θ,x,y))
$
模型参数：θ
模型输入，即图像：x
结果标签：y
损失函数：J(θ,x,y)
符号函数：sign() 就是激活函数
我们可以线性化代价函数的当前值θ,获得最优max-norm η
<img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 符号函数sign()是一个什么样的函数？</p>
<h2 id="kl-">kl 散度---相对熵<a class="headerlink" href="#kl-" title="Permanent link">&para;</a></h2>
<p><a href="https://blog.csdn.net/weixinhum/article/details/85064685">https://blog.csdn.net/weixinhum/article/details/85064685</a></p>
<p>在信息论中，相对熵等价于两个概率分布的信息熵的差值，若其中一个概率分布为真实分布，另一个为理论（拟合）分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生**的信息损耗** </p>
<div>
<div class="MathJax_Preview">
D_{K L}(p \| q)=\sum_{i=1}^{N}\left[p\left(x_{i}\right) \log p\left(x_{i}\right)-p\left(x_{i}\right) \log q\left(x_{i}\right)\right]
</div>
<script type="math/tex; mode=display">
D_{K L}(p \| q)=\sum_{i=1}^{N}\left[p\left(x_{i}\right) \log p\left(x_{i}\right)-p\left(x_{i}\right) \log q\left(x_{i}\right)\right]
</script>
</div>
<p>这里的p（x),q(X)分别表示真实时间的概率和理论拟合出来的概率</p>
<p>作用：</p>
<p>该公式得出来的解要么=0,要么大于0</p>
<p>很显然,p == q的时候，公式等于0</p>
<p>其他情况,公式大于0（证明在最后5）</p>
<p>但它有不好的地方，就是它是不对称的。举个例子，比如随机变量<span><span class="MathJax_Preview">X∼p</span><script type="math/tex">X∼p</script></span>取值为1,2,3时的概率分别为[0.1,0.4,0.5]，随机变量Y∼Q取值为1,2,3时的概率分别为[0.4,0.2,0.4]，则：
$$
\begin{array}{l}
D(P | Q)=0.1 \times \log \left(\frac{0.1}{0.4}\right)+0.4 \times \log \left(\frac{0.4}{0.2}\right)+0.5 \times \log \left(\frac{0.5}{0.4}\right)=0.250 \
D(Q | P)=0.4 \times \log \left(\frac{0.4}{0.1}\right)+0.2 \times \log \left(\frac{0.2}{0.4}\right)+0.4 \times \log \left(\frac{0.4}{0.5}\right)=0.327
\end{array}
$$</p>
<p>也就是用P来拟合Q和用Q来拟合P的相对熵居然不一样，而他们的距离是一样的。这也就是说，<strong>相对熵的大小并不跟距离有一一对应的关系</strong>。这点蛮头疼的，因为一般我们希望距离越远下降越快，而相对熵取哪个为参考在同等距离情况下下降的速度都不一样，这就非常尴尬了。</p>
<p>5证明步骤：</p>
<p><img alt="image-20200724124817276" src="https://gitee.com/moluggg/image/raw/master/img/202008/02/105313-809790.png" /></p>
<h2 id="ngcf">NGCF<a class="headerlink" href="#ngcf" title="Permanent link">&para;</a></h2>
<h3 id="1_2">1.背景<a class="headerlink" href="#1_2" title="Permanent link">&para;</a></h3>
<p>​   <mark><strong>对user和item的向量表示（如embedding）是现代推荐系统的核心</strong></mark> 从早期的矩阵分解到最近出现的基于我的深度学习这些，现有的努力通常通过从描述用户（或项目）的预先存在的特征(如ID和属性)映射来获得用户（或项目）的嵌入。 我们认为这是个继承者 这种方法的缺点是，<strong>user-item中潜在的交互collaborative signal没有嵌入</strong>。 因此，结果嵌入可能不足够捕捉协同**过滤效应**。 </p>
<p><img alt="💯" class="emojione" src="https://cdnjs.cloudflare.com/ajax/libs/emojione/2.2.7/assets/png/1f4af.png" title=":100:" /> 什么是二部图？</p>
<p>​   在本工作中，我们建议将用户项交互(-更具体地说是-)二部图结构-集成到嵌入过程中。 我们知道 开发了一种新的推荐框架神经图协同过滤(NGCF)，它**通过传播嵌入来利用用户项图结构**。 这导致了表现性的建模 用户项图中的==高阶连通性==，<strong>有效地将协作信号以显式的方式注入嵌入过程</strong>。 我们对三个公共基准进行了广泛的实验展示了对几种最先进的模型的显著改进，如HOPRec[40]和协作内存网络[5]。 进一步分析验证了嵌入传播f的重要性或者学习更好的用户和项目表示，证明NGCF的合理性和有效性。 守则可在https://github.com/xiangwang1223/neural_graph_collaborative_filtering.查阅</p>
<p>​    个性化推荐无处不在，已经应用于电子商务、广告和社交媒体等许多在线服务。 <strong>其核心是评估基于用户的历史和互动预测用户是否会选中物品</strong>，用户的历史和互动如购买和点击等。 协作过滤CF的解决措施是： 通过假设行为相似的用户在项目上表现出相似的偏好来解决它。为了实现这个假设，通常通过 <mark>参数化</mark> user和item来重建历史交互，并且基于参数去预测用户的偏好</p>
<p>一般来说 在可学习的CF模型中有两个关键因素：</p>
<p>1)嵌入，将用户和项目转换为embedding表示</p>
<p>2)交互建模，重建基于embedding的历史interactions 问题。  </p>
<blockquote>
<p>MF用内积直接嵌入；collaborative deep learning从item的边信息中整合深度学习到的representations（embedding function ）；neural collaborative filtering 用非线性神经网络（interaction function）；translation-basedCF用Euclidean distance metric（interaction function）。</p>
</blockquote>
<div class="codehilite"><pre><span></span><code>尽管它们是有效的，但我们认为这些方法不足以为CF产生令人满意的嵌入。 关键原因是embedding function 缺乏对**collaborative signal**的 ==显式的编码，== 这种signal是在user-item之间潜在的交互，它可以反映users（or items）间的行为相似度，大部分现存方法建立embedding function仅仅使用表面特征(例如ID和属性)而不考虑user-item间的交互，所以是低效的。

虽然直观地将用户项交互集成到嵌入函数中很有用，但要做好它是不平凡的。 特别是，相互作用的规模很容易达到数百万甚至上千万，因此在实际应用中，很难提取所需的协作信号。 在本工作中，我们通过利用用户项交互的高阶连接（ **high-order connectivity** ）来解决这个挑战， 在交互图结构中编码协作信号的一种自然方法。
</code></pre></div>


<p><img alt="image-20200806101900959" src="https://gitee.com/moluggg/image/raw/master/img/202008/06/101902-742549.png" /></p>
<p>运行示例。图 1 展示了一个 user-item 的二部图及 u1 的高阶连接性。u1 的高阶连接性表示 u1 通过长度大于 1 的路径连接到的节点。例如，u1 通过长度 l=2 的路径连接到 u2 和 u3，这代表 u1 的 2 阶连接性；u1 通过长度 l=3 的路径连接到 i4，i5，这代表 u1 的 3 阶连接性。需要注意的是，虽然 i4 和 i5 都是 u1 的 3 阶邻居，但是 i4 可以通过更多的路径连接到 u1，所以 i4 与 u1 的相似度更高。</p>
<p>目前的工作。作者设计了一个**嵌入传播层**（<strong>embedding propagation</strong>），它通过聚合交互项目（或用户)的嵌入来细化用户(或项目)的嵌入）。 比 堆叠多个嵌入传播层，我们可以强制嵌入，以捕获协作信号的高阶连接性。 以图1为例，堆叠两层帽 根据U1←I2←U2的行为相似性，叠加三层捕获了U1←I2←U2和I4的潜在，以及信息流的强度(由TRA估计 层之间的可变权重)确定I4和I5的推荐优先级</p>
<p>最后，值得一提的是，虽然在最近的一种名为HOP-Rec[40]的方法中考虑了高阶连通性信息，但它只是用来丰富训练数据。 具体来说，HOPRec的预测模型仍然是MF，而它是通过优化具有高阶连接性的损失来训练的。 与HOP-Rec不同的是，我们贡献了一个新的TEC 将高阶连接性集成到预测模型中，这在经验上比CF的HOP-Rec产生了更好的嵌入</p>
<p>总之，本工作做出了以下主要贡献：</p>
<p>（1） 在基于模型的CF方法的嵌入函数中，我们强调了显式利用协同信号的重要性。</p>
<p>（2） 作者提出了一种新的基于图神经网络的推荐框架NGCF，它通过执行嵌入，以高阶连接的形式显式地编码协作信号传播。</p>
<p>（3） 我们对300万个数据集进行实证研究。 广泛的结果证明了NGCF的最新性能及其在改进嵌入中的有效性 与神经嵌入传播的相关性。</p>
<h3 id="2-methodology"><strong>2 METHODOLOGY</strong><a class="headerlink" href="#2-methodology" title="Permanent link">&para;</a></h3>
<p>我们现在提出了所提出的NGCF模型，其体系结构如图2所示。 框架中有三个组件：</p>
<p>（1）提供和初始化使用的嵌入层用户 嵌入和项嵌入；</p>
<p>（2）通过注入高阶连通性关系来细化嵌入的多个嵌入传播层；</p>
<p>（3）从不同的传播层聚合细化的嵌入并输出用户项对的亲和力分数的预测层。 最后，我们讨论了NGCF的时间复杂度以及与现有方法的联系。</p>
<p>概括来说：</p>
<p>NGCF模型主要包含三个部分：
（1）Embedding Layer：将 user 和 item 的 ID 映射为向量初始化表示；
（2）Embedding Propagation Layers：联合高阶连通性来更新embedding
（3）Prediction：聚合来自传播层（Embedding Propagation Layers）的更新embedding，输出预测结果</p>
<p><img alt="image-20200806112909865" src="https://gitee.com/moluggg/image/raw/master/img/202008/06/112912-931919.png" /></p>
<h4 id="21-embedding-layer"><strong>2.1 Embedding Layer</strong><a class="headerlink" href="#21-embedding-layer" title="Permanent link">&para;</a></h4>
<p>在[1,14,26]的主流推荐模型之后，我们描述了一个带有嵌入向量Eu∈RD(EI∈RD)的用户u(一个项目I)，其中d表示嵌入大小。 这可以看作是建立一个参数矩阵作为嵌入查找表：</p>
<div>
<div class="MathJax_Preview">
\mathbf{E}=[\underbrace{\mathbf{e}_{u_{1}}, \cdots, \mathbf{e}_{u_{N}}}_{\text {users embeddings }}, \underbrace{\mathbf{e}_{i_{1}}, \cdots, \mathbf{e}_{i_{M}}}_{\text {item embeddings }}]
</div>
<script type="math/tex; mode=display">
\mathbf{E}=[\underbrace{\mathbf{e}_{u_{1}}, \cdots, \mathbf{e}_{u_{N}}}_{\text {users embeddings }}, \underbrace{\mathbf{e}_{i_{1}}, \cdots, \mathbf{e}_{i_{M}}}_{\text {item embeddings }}]
</script>
</div>
<p>值得注意的是，<strong>这个嵌入表作为用户嵌入和项嵌入的初始状态，以 <mark>端到端的方式</mark> 进行优化。</strong> 在传统的推荐模式，如MFAN 神经协作过滤[14]，将这些ID嵌入直接输入到交互层（或算子）中，以获得预测分数。 相反， <mark>在我们的NGCF框架中，我们细化了通过在user-item交互图上传播它们来进行embedings。</mark>   这导致了更有效的推荐嵌入，因为嵌入细化步骤显式地将协作信号注入嵌入。</p>
<h4 id="22-embedding-propagation-layers"><strong>2.2 Embedding Propagation Layers</strong><a class="headerlink" href="#22-embedding-propagation-layers" title="Permanent link">&para;</a></h4>
<p>接下来，我们构建了GNN的消息传递体系结构以便沿着图结构捕获CF信号并细化用户和项的嵌入。 我们首先说明设计 一层传，然后将其推广到多个连续层。</p>
<h5 id="221-first-order-propagation">2.2.1  First-order Propagation<a class="headerlink" href="#221-first-order-propagation" title="Permanent link">&para;</a></h5>
<p>直观上，与用户交互过的项目可以体现用户的偏好，类似的，与item 交互过的用户也可以看作是item的特性，并且可以用于度量两个项目之间的协作相似性,为此气门需要将 user-item 联系起来</p>
<p>那么定义一个 user-item 对 ，定义<code>从 i 到 u 的信息嵌入</code> mu←i为：</p>
<div>
<div class="MathJax_Preview">
\mathbf{m}_{u \leftarrow i}=\frac{1}{\sqrt{\left|\mathcal{N}_{u}\right|\left|\mathcal{N}_{i}\right|}}\left(\mathbf{W}_{1} \mathbf{e}_{i}+\mathbf{W}_{2}\left(\mathbf{e}_{i} \odot \mathbf{e}_{u}\right)\right)
</div>
<script type="math/tex; mode=display">
\mathbf{m}_{u \leftarrow i}=\frac{1}{\sqrt{\left|\mathcal{N}_{u}\right|\left|\mathcal{N}_{i}\right|}}\left(\mathbf{W}_{1} \mathbf{e}_{i}+\mathbf{W}_{2}\left(\mathbf{e}_{i} \odot \mathbf{e}_{u}\right)\right)
</script>
</div>
<p>可以概括表示成：</p>
<div>
<div class="MathJax_Preview">
\mathbf{m}_{u \leftarrow i}=f\left(\mathbf{e}_{i}, \mathbf{e}_{u}, p_{u i}\right)
</div>
<script type="math/tex; mode=display">
\mathbf{m}_{u \leftarrow i}=f\left(\mathbf{e}_{i}, \mathbf{e}_{u}, p_{u i}\right)
</script>
</div>
<p>这样就可以 在此基础上在连接的user 与 item 之间执行**嵌入传播**。</p>
<p>公式说明：</p>
<p>① <span><span class="MathJax_Preview">m_{u←i}</span><script type="math/tex">m_{u←i}</script></span>是消息embedding，f(⋅)是消息编码函数，ei和eu作为输入，利用系数pui控制边(u,i)上每次传播的衰减因子。</p>
<p>②  <span><span class="MathJax_Preview">p_{ui}=\frac{1}{\sqrt{\left|\mathcal{N}_{u}\right|\left|\mathcal{N}_{i}\right|}}</span><script type="math/tex">p_{ui}=\frac{1}{\sqrt{\left|\mathcal{N}_{u}\right|\left|\mathcal{N}_{i}\right|}}</script></span>  表示的是衰减因子，也是正则化系数，表示历史item对用户偏好的贡献程度，从消息传递的观点看，<span><span class="MathJax_Preview">p_{ui}</span><script type="math/tex">p_{ui}</script></span>被看成是一个折扣因子，表示消息在传播过程中的衰退。</p>
<p>*③ W*1,*W*2是d′∗d维的权值矩阵，去提取有用信息进行传播。</p>
<p>④ 公式特色： 传统的图神经网络只是考虑 ei 的作用，本文中我们通过ei⊙eu把ei和eu之间的交互编码到传递的信息中，这使得消息依赖于ei和eu之间的关联性，传递更多来自相似items的消息。</p>
<p>上述说明了单层传播的设计，现在将其推广到多个连续层</p>
<h5 id="222-high-order-propagation"><em>2.2.2</em>  <strong>High-order Propagation</strong><a class="headerlink" href="#222-high-order-propagation" title="Permanent link">&para;</a></h5>
<p>这样就可以聚合从u的邻居传来的信息去更新u的表示，聚合函数为：</p>
<div>
<div class="MathJax_Preview">
\mathbf{e}_{u}^{(1)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}\right)
</div>
<script type="math/tex; mode=display">
\mathbf{e}_{u}^{(1)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}\right)
</script>
</div>
<p>上述公式是一阶嵌入传播层，除了从邻居 Nu 传播的消息外，还考虑了 u 的自连接:mu←u = W1eu，保留了原始特征的信息。同理$ e_{i}^{(1)}$ </p>
<p>综合上述（12）~（14）：</p>
<div>
<div class="MathJax_Preview">
\mathbf{e}_{u}^{(l)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}^{(l)}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}^{(l)}\right)\left\{\begin{array}{l}
\mathbf{m}_{u \leftarrow i}^{(l)}=p_{u i}\left(\mathbf{W}_{1}^{(l)} \mathbf{e}_{i}^{(l-1)}+\mathbf{W}_{2}^{(l)}\left(\mathbf{e}_{i}^{(l-1)} \odot \mathbf{e}_{u}^{(l-1)}\right)\right) \\
(l) \\
\mathbf{m}_{u \leftarrow u}=\mathbf{W}_{1}^{(l)} \mathbf{e}_{u}^{(l-1)}
\end{array}\right.
</div>
<script type="math/tex; mode=display">
\mathbf{e}_{u}^{(l)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}^{(l)}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}^{(l)}\right)\left\{\begin{array}{l}
\mathbf{m}_{u \leftarrow i}^{(l)}=p_{u i}\left(\mathbf{W}_{1}^{(l)} \mathbf{e}_{i}^{(l-1)}+\mathbf{W}_{2}^{(l)}\left(\mathbf{e}_{i}^{(l-1)} \odot \mathbf{e}_{u}^{(l-1)}\right)\right) \\
(l) \\
\mathbf{m}_{u \leftarrow u}=\mathbf{W}_{1}^{(l)} \mathbf{e}_{u}^{(l-1)}
\end{array}\right.
</script>
</div>
<p>同理，<span><span class="MathJax_Preview">e_{i}^{(l)}</span><script type="math/tex">e_{i}^{(l)}</script></span> 也有类似的表示。</p>
<p>公式说明：</p>
<p>① 与2.2.1的公式相似 ，理解与2.2.1基本一样</p>
<p>② 可以通过以下一个例子进行理解</p>
<p>1.一阶连接性可以使得表示进一步准确，可以通过堆叠嵌入传播层去扩展高阶连接性信息，<strong>高阶连接性对于编码协同信号去估计用户和商品间的关联性来说是很重要的。</strong>
2.通过堆叠l个嵌入传播层一个用户能够收到它的l跳邻居传来的消息，如图所示，在第l步，用户u的递归表示为</p>
<p><img alt="image-20200807111428284" src="https://gitee.com/moluggg/image/raw/master/img/202008/07/111451-213764.png" /></p>
<p>如图三所示，协同信号例如u1←i2←u2←i4可以被嵌入传播过程捕捉，来自i4的消息被显式的编码在<span><span class="MathJax_Preview">e_{u1}^{(3)}</span><script type="math/tex">e_{u1}^{(3)}</script></span>中</p>
<p><img alt="Alt" src="https://gitee.com/moluggg/image/raw/master/img/202008/07/112425-938930.png" /></p>
<p>为了提供嵌入传播的整体视图，方便批量实现，提供了分层传播规则的矩阵形式:</p>
<div>
<div class="MathJax_Preview">
\mathbf{E}^{(l)}=\operatorname{LeakyReLU}\left((\mathcal{L}+\mathbf{I}) \mathbf{E}^{(l-1)} \mathbf{W}_{1}^{(l)}+\mathcal{L} \mathbf{E}^{(l-1)} \odot \mathbf{E}^{(l-1)} \mathbf{W}_{2}^{(l)}\right)
</div>
<script type="math/tex; mode=display">
\mathbf{E}^{(l)}=\operatorname{LeakyReLU}\left((\mathcal{L}+\mathbf{I}) \mathbf{E}^{(l-1)} \mathbf{W}_{1}^{(l)}+\mathcal{L} \mathbf{E}^{(l-1)} \odot \mathbf{E}^{(l-1)} \mathbf{W}_{2}^{(l)}\right)
</script>
</div>
<p>公式：</p>
<p>①E 是用户和项经过 l 步嵌入传播后得到的表示
②I 表示一个单位矩阵
③ L 表示用户-项目图的拉普拉斯矩阵:</p>
<div>
<div class="MathJax_Preview">
\mathcal{L}=\mathbf{D}^{-\frac{1}{2}} \mathbf{A D}^{-\frac{1}{2}} \text { and } \mathbf{A}=\left[\begin{array}{cc}
\mathbf{0} &amp; \mathbf{R} \\
\mathbf{R}^{\top} &amp; \mathbf{0}
\end{array}\right]
</div>
<script type="math/tex; mode=display">
\mathcal{L}=\mathbf{D}^{-\frac{1}{2}} \mathbf{A D}^{-\frac{1}{2}} \text { and } \mathbf{A}=\left[\begin{array}{cc}
\mathbf{0} & \mathbf{R} \\
\mathbf{R}^{\top} & \mathbf{0}
\end{array}\right]
</script>
</div>
<h4 id="23-model-prediction">2.3  Model Prediction<a class="headerlink" href="#23-model-prediction" title="Permanent link">&para;</a></h4>
<p>由于在不同层中获得的表示强调通过不同连接传递的消息，所以它们在反映用户偏好方面有不同的贡献。
因此，将它们串联起来，构成用户的最终嵌入；对 item 也做同样的操作。
<img alt="在这里插入图片描述" src="https://gitee.com/moluggg/image/raw/master/img/202008/07/122222-394690.png" />
其中||为串联操作。除了连接，其他聚合器也可以应用，如加权平均、最大池、LSTM。使用串联在于它的简单性，不需要学习额外的参数，而且已经被非常有效地证明了。
最后，我们进行内积来估计用户对目标物品的偏好:
<img alt="在这里插入图片描述" src="https://gitee.com/moluggg/image/raw/master/img/202008/07/122229-358156.png" /></p>
<h4 id="24-optimization">2.4 Optimization<a class="headerlink" href="#24-optimization" title="Permanent link">&para;</a></h4>
<p><strong>optimize the pairwise BPR loss</strong>
它考虑观察到的和未观察到的用户-项目交互之间的相对顺序。具体地说，BPR 假设用户引用的已观察到的交互作用应该比未观察到的交互作用具有更高的预测值。目标函数如下：
<img alt="在这里插入图片描述" src="https://gitee.com/moluggg/image/raw/master/img/202008/07/121805-316907.png" /></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../d_GCN论文解读/" title="GCN" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                GCN
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.5e60981f.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="../js/extra.js"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
    
      
    
  </body>
</html>